import sys
import os
import time
import random
import csv
import re
import shutil
from datetime import datetime

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)
sys.path.insert(0, os.path.join(current_dir, 'coupang'))
sys.path.insert(0, os.path.join(current_dir, 'iherbscraper'))

from coupang.coupang_manager import BrowserManager as CoupangBrowser
from iherbscraper.iherb_manager import BrowserManager as IHerbBrowser
from iherbscraper.iherb_client import IHerbClient
from iherbscraper.product_matcher import ProductMatcher

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class InfiniteScrollCrawler:
    """ë¬´í•œ ìŠ¤í¬ë¡¤ í¬ë¡¤ëŸ¬ - ë””ë²„ê¹… í¬í•¨"""
    
    def __init__(self, browser, existing_ids, global_new_ids):
        self.browser = browser
        self.existing_ids = existing_ids
        self.global_new_ids = global_new_ids
        self.products = []
    
    def infinite_scroll_crawl(self, url, target_count=40):
        """ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ì‹ ê·œ ìƒí’ˆë§Œ ìˆ˜ì§‘"""
        try:
            print(f"\n{'='*80}")
            print(f"ğŸ¯ ì‹ ê·œ ìƒí’ˆ í¬ë¡¤ë§: {target_count}ê°œ ëª©í‘œ")
            print(f"{'='*80}\n")
            
            self.browser.driver.get(url)
            time.sleep(random.uniform(3, 5))
            
            self._click_sales_filter()
            
            no_new_products_count = 0
            max_no_new_attempts = 15
            category_seen_ids = set()
            scroll_count = 0
            max_scrolls = 200
            consecutive_no_height_change = 0
            
            while len(self.products) < target_count and scroll_count < max_scrolls:
                scroll_count += 1
                
                collected = self._extract_new_products(category_seen_ids)
                
                progress = (len(self.products) / target_count) * 100
                print(f"[ìŠ¤í¬ë¡¤ {scroll_count}íšŒ] ì‹ ê·œ: {collected}ê°œ | ëˆ„ì : {len(self.products)}/{target_count}ê°œ ({progress:.1f}%)")
                
                if collected > 0:
                    no_new_products_count = 0
                    consecutive_no_height_change = 0
                else:
                    no_new_products_count += 1
                
                # ëª©í‘œ ë‹¬ì„±
                if len(self.products) >= target_count:
                    print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±: {len(self.products)}ê°œ!")
                    break
                
                # ìŠ¤í¬ë¡¤
                height_changed = self._scroll_to_bottom()
                
                if not height_changed:
                    consecutive_no_height_change += 1
                    print(f"  âš ï¸ ë†’ì´ ë³€í™” ì—†ìŒ (ì—°ì† {consecutive_no_height_change}íšŒ)")
                    
                    # 5íšŒ ì—°ì† ë†’ì´ ë³€í™” ì—†ìœ¼ë©´ì„œ ì‹ ê·œë„ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if consecutive_no_height_change >= 5 and no_new_products_count >= 5:
                        print(f"âš ï¸ ë” ì´ìƒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        break
                else:
                    consecutive_no_height_change = 0
                
                # ì˜¤ë˜ ì‹ ê·œ ì—†ìœ¼ë©´ ê²½ê³ 
                if no_new_products_count >= max_no_new_attempts:
                    print(f"âš ï¸ {max_no_new_attempts}íšŒ ì—°ì† ì‹ ê·œ 0ê°œ (ê³„ì† ì‹œë„ ì¤‘...)")
                
                time.sleep(random.uniform(1, 2))
            
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(self.products)}ê°œ ìˆ˜ì§‘")
            if len(self.products) < target_count:
                print(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬: {target_count - len(self.products)}ê°œ ë¶€ì¡±")
                print(f"   â†’ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì‹ ê·œ ìƒí’ˆì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n")
            else:
                print()
            
            return self.products
            
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return self.products
    
    def _click_sales_filter(self):
        """íŒë§¤ëŸ‰ìˆœ í•„í„° í´ë¦­"""
        try:
            print("ğŸ” íŒë§¤ëŸ‰ìˆœ í•„í„° ì°¾ëŠ” ì¤‘...")
            time.sleep(2)
            
            filter_buttons = self.browser.driver.find_elements(By.CSS_SELECTOR, 'li.sortkey')
            
            for button in filter_buttons:
                if 'íŒë§¤ëŸ‰ìˆœ' in button.text:
                    button.click()
                    print("âœ… íŒë§¤ëŸ‰ìˆœ í•„í„° ì ìš©")
                    time.sleep(3)
                    return
            
            print("âš ï¸ íŒë§¤ëŸ‰ìˆœ í•„í„° ì—†ìŒ (ê¸°ë³¸ ì •ë ¬ë¡œ ì§„í–‰)")
        except Exception as e:
            print(f"âš ï¸ íŒë§¤ëŸ‰ìˆœ í•„í„° ì‹¤íŒ¨: {e}")
    
    def _extract_new_products(self, category_seen_ids):
        """ì‹ ê·œ ìƒí’ˆë§Œ ì¶”ì¶œ - ë””ë²„ê¹… í¬í•¨"""
        try:
            collected = 0
            product_elements = self.browser.driver.find_elements(By.CSS_SELECTOR, 'li.product-wrap')
            
            print(f"  ğŸ” í˜ì´ì§€ì—ì„œ ë°œê²¬í•œ ìƒí’ˆ: {len(product_elements)}ê°œ")
            
            duplicates = {'existing': 0, 'global': 0, 'category': 0}
            
            for element in product_elements:
                try:
                    link = element.find_element(By.CSS_SELECTOR, 'a.product-wrapper')
                    product_url = link.get_attribute('href')
                    
                    match = re.search(r'itemId=(\d+)', product_url)
                    if not match:
                        continue
                    
                    product_id = match.group(1)
                    
                    # ì¤‘ë³µ ì²´í¬ (ë””ë²„ê¹…)
                    if product_id in self.existing_ids:
                        duplicates['existing'] += 1
                        continue
                    
                    if product_id in self.global_new_ids:
                        duplicates['global'] += 1
                        continue
                    
                    if product_id in category_seen_ids:
                        duplicates['category'] += 1
                        continue
                    
                    # ìƒí’ˆëª…
                    try:
                        name_elem = element.find_element(By.CSS_SELECTOR, 'div.name')
                        product_name = name_elem.text.strip()
                    except:
                        continue
                    
                    # ê°€ê²©
                    try:
                        price_elem = element.find_element(By.CSS_SELECTOR, 'strong.price-value')
                        price = price_elem.text.strip()
                    except:
                        price = ""
                    
                    if not product_name:
                        continue
                    
                    # ì‹ ê·œ ìƒí’ˆ ì¶”ê°€
                    self.products.append({
                        'product_id': product_id,
                        'product_name': product_name,
                        'product_url': product_url,
                        'current_price': price
                    })
                    
                    category_seen_ids.add(product_id)
                    self.global_new_ids.add(product_id)
                    collected += 1
                    
                    # ì²« ì‹ ê·œ ìƒí’ˆ ë¡œê·¸
                    if collected == 1:
                        print(f"  âœ… ì²« ì‹ ê·œ ìƒí’ˆ: {product_name[:30]}... (ID: {product_id})")
                    
                except:
                    continue
            
            # ì¤‘ë³µ í†µê³„
            if duplicates['existing'] > 0 or duplicates['global'] > 0 or duplicates['category'] > 0:
                print(f"  âš ï¸ ì¤‘ë³µ: ìºì‹œ {duplicates['existing']}ê°œ | ì „ì—­ {duplicates['global']}ê°œ | ì¹´í…Œê³ ë¦¬ {duplicates['category']}ê°œ")
            
            return collected
            
        except Exception as e:
            print(f"  âŒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return 0
    
    def _scroll_to_bottom(self):
        """ìŠ¤í¬ë¡¤ & ìƒˆ ì½˜í…ì¸  í™•ì¸"""
        try:
            last_height = self.browser.driver.execute_script("return document.body.scrollHeight")
            
            # ì²œì²œíˆ ë‹¨ê³„ì ìœ¼ë¡œ ìŠ¤í¬ë¡¤
            scroll_steps = random.randint(3, 5)
            for i in range(scroll_steps):
                scroll_amount = (last_height / scroll_steps) * (i + 1)
                self.browser.driver.execute_script(f"window.scrollTo(0, {scroll_amount});")
                time.sleep(0.3)
            
            # ì™„ì „íˆ ëê¹Œì§€
            self.browser.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ìƒˆ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
            for _ in range(5):
                time.sleep(1)
                new_height = self.browser.driver.execute_script("return document.body.scrollHeight")
                if new_height > last_height:
                    return True
            
            return False
            
        except:
            return False


class CoupangIHerbCrawler:
    """ì¿ íŒ¡ -> ì•„ì´í—ˆë¸Œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, 
                 output_file="coupang_iherb_products.csv",
                 cache_file="coupang_products_cache.csv",
                 restart_interval=10):
        """
        Args:
            output_file: ë§¤ì¹­ ê²°ê³¼ CSV
            cache_file: ì¿ íŒ¡ ìƒí’ˆ ìºì‹œ
            restart_interval: Nê°œë§ˆë‹¤ ë¸Œë¼ìš°ì € ì¬ì‹œì‘
        """
        self.output_file = output_file
        self.cache_file = cache_file
        self.restart_interval = restart_interval
        
        self.csv_file = None
        self.csv_writer = None
        
        # ë°ì´í„°
        self.cache_product_ids = set()
        self.matched_product_ids = set()
        self.all_cache_products = []
        self.new_products = []
        self.global_new_ids = set()
        
        # ë¸Œë¼ìš°ì €
        self.coupang_browser = None
        self.iherb_browser = None
        self.iherb_client = None
        self.product_matcher = None
        
        self._load_data()
        self._init_csv()
    
    def _load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        # ìºì‹œ ë¡œë“œ
        if os.path.exists(self.cache_file):
            print(f"ğŸ“‚ ìºì‹œ ë¡œë“œ: {self.cache_file}")
            try:
                with open(self.cache_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        product_id = row.get('ìƒí’ˆID', '')
                        if product_id:
                            self.cache_product_ids.add(product_id)
                        self.all_cache_products.append(row)
                
                total_rows = len(self.all_cache_products)
                unique_ids = len(self.cache_product_ids)
                duplicates = total_rows - unique_ids
                
                print(f"  âœ… ìºì‹œ ì „ì²´: {total_rows}ê°œ")
                print(f"  âœ… ìœ ë‹ˆí¬ ìƒí’ˆ: {unique_ids}ê°œ")
                if duplicates > 0:
                    print(f"  âš ï¸ ì¤‘ë³µ: {duplicates}ê°œ")
            except Exception as e:
                print(f"  âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ
        if os.path.exists(self.output_file):
            print(f"ğŸ“‚ ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ: {self.output_file}")
            try:
                with open(self.output_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        url = row.get('ì¿ íŒ¡_ìƒí’ˆURL', '')
                        match = re.search(r'itemId=(\d+)', url)
                        if match:
                            self.matched_product_ids.add(match.group(1))
                
                print(f"  âœ… ë§¤ì¹­ ì™„ë£Œ: {len(self.matched_product_ids)}ê°œ\n")
            except Exception as e:
                print(f"  âš ï¸ ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}\n")
    
    def _init_csv(self):
        """CSV ì´ˆê¸°í™”"""
        file_exists = os.path.exists(self.output_file)
        
        if file_exists:
            self.csv_file = open(self.output_file, 'a', newline='', encoding='utf-8-sig')
            self.csv_writer = csv.DictWriter(self.csv_file, fieldnames=[
                'ìˆ˜ì§‘ìˆœì„œ', 'ì¹´í…Œê³ ë¦¬', 'ì¿ íŒ¡_ìƒí’ˆURL', 'ì¿ íŒ¡_ì œí’ˆëª…',
                'ì¿ íŒ¡_ë¹„íšŒì›ê°€ê²©', 'ì•„ì´í—ˆë¸Œ_UPC', 'ì•„ì´í—ˆë¸Œ_íŒŒíŠ¸ë„˜ë²„', 'ìˆ˜ì§‘ì‹œê°„'
            ])
            print(f"ğŸ“ CSV íŒŒì¼ ì—´ê¸°: {self.output_file}\n")
        else:
            self.csv_file = open(self.output_file, 'w', newline='', encoding='utf-8-sig')
            self.csv_writer = csv.DictWriter(self.csv_file, fieldnames=[
                'ìˆ˜ì§‘ìˆœì„œ', 'ì¹´í…Œê³ ë¦¬', 'ì¿ íŒ¡_ìƒí’ˆURL', 'ì¿ íŒ¡_ì œí’ˆëª…',
                'ì¿ íŒ¡_ë¹„íšŒì›ê°€ê²©', 'ì•„ì´í—ˆë¸Œ_UPC', 'ì•„ì´í—ˆë¸Œ_íŒŒíŠ¸ë„˜ë²„', 'ìˆ˜ì§‘ì‹œê°„'
            ])
            self.csv_writer.writeheader()
            print(f"âœ… CSV íŒŒì¼ ìƒì„±: {self.output_file}\n")
        
        self.csv_file.flush()
    
    def crawl_new_products(self, categories):
        """ì‹ ê·œ ì¿ íŒ¡ ìƒí’ˆ í¬ë¡¤ë§"""
        print("\n" + "="*80)
        print("ğŸ“¦ ì¿ íŒ¡ í¬ë¡¤ë§")
        print("="*80)
        
        self.coupang_browser = CoupangBrowser(headless=False)
        if not self.coupang_browser.start_driver():
            raise Exception("ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨")
        
        crawler = InfiniteScrollCrawler(
            self.coupang_browser,
            self.cache_product_ids,
            self.global_new_ids
        )
        
        for category in categories:
            print(f"\nğŸ“‚ {category['name']}: {category['count']}ê°œ ëª©í‘œ")
            
            crawler.products = []
            products = crawler.infinite_scroll_crawl(category['url'], category['count'])
            
            if products:
                for p in products:
                    p['category'] = category['name']
                self.new_products.extend(products)
                print(f"âœ… {len(products)}ê°œ ìˆ˜ì§‘")
            else:
                print(f"âš ï¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì‹ ê·œ ìƒí’ˆ ì—†ìŒ)")
        
        self.coupang_browser.close()
        print(f"\nâœ… ì´ {len(self.new_products)}ê°œ ì‹ ê·œ ìˆ˜ì§‘\n")
        
        self._update_cache()
    
    def _update_cache(self):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        if not self.new_products:
            return
        
        # ë°±ì—…
        if os.path.exists(self.cache_file):
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup = f"{self.cache_file}.{timestamp}.backup"
            shutil.copy(self.cache_file, backup)
            print(f"ğŸ’¾ ë°±ì—…: {backup}")
        
        # ì¶”ê°€
        for p in self.new_products:
            self.all_cache_products.append({
                'ì¹´í…Œê³ ë¦¬': p['category'],
                'ìƒí’ˆID': p['product_id'],
                'ìƒí’ˆëª…': p['product_name'],
                'ìƒí’ˆURL': p['product_url'],
                'ê°€ê²©': p['current_price']
            })
        
        # ì €ì¥
        try:
            with open(self.cache_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=['ì¹´í…Œê³ ë¦¬', 'ìƒí’ˆID', 'ìƒí’ˆëª…', 'ìƒí’ˆURL', 'ê°€ê²©'])
                writer.writeheader()
                
                # ê¸°ì¡´ ë°ì´í„° ì •ë¦¬ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
                clean_rows = []
                for row in self.all_cache_products:
                    clean_row = {
                        'ì¹´í…Œê³ ë¦¬': row.get('ì¹´í…Œê³ ë¦¬', ''),
                        'ìƒí’ˆID': row.get('ìƒí’ˆID', ''),
                        'ìƒí’ˆëª…': row.get('ìƒí’ˆëª…', ''),
                        'ìƒí’ˆURL': row.get('ìƒí’ˆURL', ''),
                        'ê°€ê²©': row.get('ê°€ê²©', '')
                    }
                    clean_rows.append(clean_row)
                
                writer.writerows(clean_rows)
            
            print(f"ğŸ’¾ ìºì‹œ ì—…ë°ì´íŠ¸: {len(clean_rows)}ê°œ\n")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}\n")
    
    def match_iherb_products(self):
        """ì•„ì´í—ˆë¸Œ ë§¤ì¹­"""
        print("\n" + "="*80)
        print("ğŸ” ì•„ì´í—ˆë¸Œ ë§¤ì¹­")
        print("="*80 + "\n")
        
        self.iherb_browser = IHerbBrowser(headless=False)
        self.iherb_client = IHerbClient(self.iherb_browser)
        self.product_matcher = ProductMatcher(self.iherb_client)
        print("âœ… ì•„ì´í—ˆë¸Œ ë¸Œë¼ìš°ì € ì¤€ë¹„\n")
        
        start_time = time.time()
        completed = 0
        restart_counter = 0
        skipped = 0
        
        for idx, product in enumerate(self.all_cache_products, 1):
            product_id = product.get('ìƒí’ˆID', '')
            
            # ì´ë¯¸ ë§¤ì¹­ ì™„ë£Œ
            if product_id in self.matched_product_ids:
                skipped += 1
                if skipped <= 3:
                    print(f"[{idx}/{len(self.all_cache_products)}] â­ï¸ ìŠ¤í‚µ: {product_id}")
                elif skipped == 4:
                    print(f"... (ì´í›„ ìŠ¤í‚µ ë¡œê·¸ ìƒëµ)")
                continue
            
            # ì§„í–‰ë¥ 
            if completed > 0:
                elapsed = time.time() - start_time
                avg = elapsed / completed
                remaining = (len(self.all_cache_products) - idx + 1) * avg
                h = int(remaining // 3600)
                m = int((remaining % 3600) // 60)
                print(f"\nâ±ï¸ ì˜ˆìƒ ì”ì—¬: {h}h {m}m | ì¬ì‹œì‘ê¹Œì§€: {self.restart_interval - restart_counter}ê°œ")
            
            try:
                success = self._match_and_save(
                    idx,
                    product.get('ì¹´í…Œê³ ë¦¬', ''),
                    product.get('ìƒí’ˆURL', ''),
                    product.get('ìƒí’ˆëª…', ''),
                    product.get('ê°€ê²©', ''),
                    product_id
                )
                
                if success:
                    completed += 1
                    restart_counter += 1
                
                # ë¸Œë¼ìš°ì € ì¬ì‹œì‘
                if restart_counter >= self.restart_interval:
                    print("\n" + "="*80)
                    print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘")
                    print("="*80)
                    
                    try:
                        self.iherb_browser.close()
                    except:
                        pass
                    
                    time.sleep(3)
                    
                    self.iherb_browser = IHerbBrowser(headless=False)
                    self.iherb_client = IHerbClient(self.iherb_browser)
                    self.product_matcher = ProductMatcher(self.iherb_client)
                    print("âœ… ì¬ì‹œì‘ ì™„ë£Œ\n")
                    
                    restart_counter = 0
                    
            except KeyboardInterrupt:
                print(f"\n{'='*80}")
                print(f"âš ï¸ ì¤‘ë‹¨")
                print(f"{'='*80}")
                print(f"ì²˜ë¦¬: {completed}ê°œ | ìŠ¤í‚µ: {skipped}ê°œ")
                print(f"ğŸ’¾ ì§„í–‰ì‚¬í•­ ì €ì¥ë¨")
                print(f"{'='*80}")
                return
        
        self.iherb_browser.close()
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ë§¤ì¹­ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"ì²˜ë¦¬: {completed}ê°œ | ìŠ¤í‚µ: {skipped}ê°œ")
        print(f"{'='*80}")

    def _match_and_save(self, idx, category, url, name, price, product_id):
        """ë§¤ì¹­ & ì €ì¥"""
        print(f"\n[{idx}/{len(self.all_cache_products)}] {name[:50]}...")
        
        upc = ""
        part_number = ""
        
        try:
            result = self.product_matcher.search_product_enhanced(name, product_id)
            
            if result and len(result) == 3:
                matched_url, score, details = result
                
                if matched_url:
                    print(f"  âœ… ë§¤ì¹­: {matched_url[:50]}...")
                    
                    code, iherb_name, price_info = \
                        self.iherb_client.extract_product_info_with_price(matched_url)
                    
                    if code:
                        part_number = code
                        print(f"  ğŸ“¦ íŒŒíŠ¸ë„˜ë²„: {part_number}")
                        
                        upc = self._extract_upc()
                        if upc:
                            print(f"  ğŸ”¢ UPC: {upc}")
                else:
                    reason = details.get('reason', 'unknown') if isinstance(details, dict) else 'unknown'
                    print(f"  âŒ ì‹¤íŒ¨: {reason}")
        
        except Exception as e:
            error = str(e)
            
            if "quota" in error.lower() or "limit" in error.lower():
                print(f"\nâš ï¸ API í• ë‹¹ëŸ‰ ì´ˆê³¼!")
                raise KeyboardInterrupt("API ì œí•œ")
            
            print(f"  âŒ ì˜¤ë¥˜: {error[:50]}...")
        
        # ì €ì¥
        row = {
            'ìˆ˜ì§‘ìˆœì„œ': idx,
            'ì¹´í…Œê³ ë¦¬': category,
            'ì¿ íŒ¡_ìƒí’ˆURL': url,
            'ì¿ íŒ¡_ì œí’ˆëª…': name,
            'ì¿ íŒ¡_ë¹„íšŒì›ê°€ê²©': price,
            'ì•„ì´í—ˆë¸Œ_UPC': upc,
            'ì•„ì´í—ˆë¸Œ_íŒŒíŠ¸ë„˜ë²„': part_number,
            'ìˆ˜ì§‘ì‹œê°„': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        try:
            self.csv_writer.writerow(row)
            self.csv_file.flush()
            os.fsync(self.csv_file.fileno())
        except Exception as e:
            print(f"  âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        self.matched_product_ids.add(product_id)
        time.sleep(random.uniform(1, 2))
        
        return True
    
    def _extract_upc(self):
        """UPC ì¶”ì¶œ"""
        try:
            elements = self.iherb_browser.driver.find_elements(
                By.XPATH, 
                "//ul[@id='product-specs-list']//li[contains(text(), 'UPC')]"
            )
            
            for elem in elements:
                match = re.search(r'UPC[^:]*:\s*([0-9]{12,13})', elem.text, re.IGNORECASE)
                if match:
                    upc = match.group(1)
                    if len(upc) in [12, 13]:
                        return upc
            
            return ""
        except:
            return ""
    
    def run(self, categories):
        """
        ì‹¤í–‰
        
        Args:
            categories: [
                {'name': 'í—¬ìŠ¤/ê±´ê°•ì‹í’ˆ', 'url': '...', 'count': 20},
            ]
        """
        try:
            # ìƒíƒœ í™•ì¸
            cache_total = len(self.cache_product_ids)
            matched_total = len(self.matched_product_ids)
            pending = cache_total - matched_total
            
            print("\n" + "="*80)
            print("ğŸ“Š í˜„ì¬ ìƒíƒœ")
            print("="*80)
            print(f"ìœ ë‹ˆí¬ ìƒí’ˆ: {cache_total}ê°œ")
            print(f"ë§¤ì¹­ ì™„ë£Œ: {matched_total}ê°œ")
            print(f"ë§¤ì¹­ ëŒ€ê¸°: {pending}ê°œ")
            print("="*80)
            
            # ë¡œì§: ë§¤ì¹­ ì™„ë£Œ â†’ í¬ë¡¤ë§ / ë¯¸ì™„ë£Œ â†’ ë§¤ì¹­
            if pending == 0:
                print(f"\nâœ… ëª¨ë“  ë§¤ì¹­ ì™„ë£Œ â†’ ì‹ ê·œ í¬ë¡¤ë§ ì‹œì‘")
                total_target = sum(c['count'] for c in categories)
                print(f"   ëª©í‘œ: {total_target}ê°œ ì¶”ê°€")
                self.crawl_new_products(categories)
                
                # í¬ë¡¤ë§ í›„ ë§¤ì¹­
                if self.new_products:
                    print(f"\nâ†’ ì‹ ê·œ ìƒí’ˆ ë§¤ì¹­ ì‹œì‘")
                    self.match_iherb_products()
                else:
                    print(f"\nâš ï¸ ì‹ ê·œ ìƒí’ˆì´ ì—†ì–´ ë§¤ì¹­ ìŠ¤í‚µ")
            else:
                print(f"\nâ­ï¸ í¬ë¡¤ë§ ìŠ¤í‚µ â†’ ê¸°ì¡´ {pending}ê°œ ë§¤ì¹­ ìš°ì„ ")
                self.match_iherb_products()
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ì¤‘ë‹¨")
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.cleanup()
        
    def cleanup(self):
        """ì •ë¦¬"""
        print("\nğŸ§¹ ì •ë¦¬ ì¤‘...")
        
        if self.csv_file:
            self.csv_file.close()
            print("  âœ… CSV ë‹«ê¸°")
        
        if self.coupang_browser:
            try:
                self.coupang_browser.close()
            except:
                pass
        
        if self.iherb_browser:
            try:
                self.iherb_browser.close()
            except:
                pass


if __name__ == "__main__":
    crawler = CoupangIHerbCrawler(
        output_file="coupang_iherb_products.csv",
        cache_file="coupang_products_cache.csv",
        restart_interval=10
    )
    
    # ì—¬ê¸°ë§Œ ìˆ˜ì •!
    categories = [
        {
            'name': 'í—¬ìŠ¤/ê±´ê°•ì‹í’ˆ',
            'url': 'https://shop.coupang.com/coupangus/74511?category=305433&platform=p&brandId=0',
            'count': 200
        },
        {
            'name': 'ì¶œì‚°ìœ ì•„ë™',
            'url': 'https://shop.coupang.com/coupangus/74511?category=219079&platform=p&brandId=0',
            'count': 200
        },
        {
            'name': 'ìŠ¤í¬ì¸ ë ˆì €',
            'url': 'https://shop.coupang.com/coupangus/74511?category=317675&platform=p&brandId=0',
            'count': 200
        }
    ]
    
    crawler.run(categories)