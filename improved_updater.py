"""
ê°„ì†Œí™”ëœ í†µí•© íŒŒì´í”„ë¼ì¸ - í•µì‹¬ ê¸°ëŠ¥ë§Œ êµ¬í˜„
- outputs í´ë” ì‚¬ìš©
- ì¤‘ë‹¨ ì§€ì ë¶€í„° ì¬ì‹œì‘ 
- ì‹¤ì‹œê°„ ì €ì¥
"""

import os
import sys
import pandas as pd
from datetime import datetime

# ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(CURRENT_DIR, 'coupang'))
sys.path.insert(0, os.path.join(CURRENT_DIR, 'iherbscraper'))
sys.path.insert(0, os.path.join(CURRENT_DIR, 'updater'))

try:
    from updater.product_updater import ProductUpdater
    from updater.data_processor import DataProcessor
except Exception as e:
    print(f"ì—…ë°ì´í„° ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

def run_pipeline(search_url: str, base_csv: str = None) -> str:
    """
    ê°„ì†Œí™”ëœ íŒŒì´í”„ë¼ì¸
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    1. outputs í´ë”ì— ëª¨ë“  ê²°ê³¼ ì €ì¥
    2. ì¿ íŒ¡ í¬ë¡¤ë§ ê²°ê³¼ ì¬ì‚¬ìš©
    3. ì•„ì´í—ˆë¸Œ ë§¤ì¹­ ì¤‘ë‹¨ì ë¶€í„° ì¬ì‹œì‘
    """
    
    # outputs ë””ë ‰í† ë¦¬ ìƒì„±
    outputs_dir = "./outputs"
    os.makedirs(outputs_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # íŒŒì¼ ê²½ë¡œ ì •ì˜
    coupang_csv = os.path.join(outputs_dir, f"coupang_crawled_{timestamp}.csv")
    final_csv = os.path.join(outputs_dir, f"final_results_{timestamp}.csv")
    
    product_updater = ProductUpdater(enable_images=True)
    data_processor = DataProcessor()
    
    try:
        # ì—…ë°ì´íŠ¸ ëª¨ë“œ ì—¬ë¶€ í™•ì¸
        if base_csv and os.path.exists(base_csv):
            print(f"ğŸ“‹ ì—…ë°ì´íŠ¸ ëª¨ë“œ")
            print(f"   ê¸°ì¡´ íŒŒì¼: {base_csv}")
            
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            base_df = pd.read_csv(base_csv, encoding='utf-8-sig')
            
            # 1. ì¿ íŒ¡ í¬ë¡¤ë§ (í•­ìƒ ìˆ˜í–‰)
            print(f"\n1ï¸âƒ£ ì¿ íŒ¡ ìµœì‹  ë°ì´í„° í¬ë¡¤ë§")
            crawled_df = product_updater.crawl_coupang_products(search_url)
            
            if len(crawled_df) == 0:
                print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
                return ""
            
            # ì¿ íŒ¡ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ (ì¬ì‚¬ìš©ì„ ìœ„í•´)
            crawled_df.to_csv(coupang_csv, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {coupang_csv}")
            
            # 2. ìƒí’ˆ ë¶„ë¥˜
            print(f"\n2ï¸âƒ£ ìƒí’ˆ ë¶„ë¥˜ (ê¸°ì¡´ vs ì‹ ê·œ)")
            updated_base_df, new_df = data_processor.classify_products(base_df, crawled_df)
            
            # 3. ì‹ ê·œ ìƒí’ˆë§Œ ë§¤ì¹­ (í•µì‹¬ ê°œì„ )
            if len(new_df) > 0:
                print(f"\n3ï¸âƒ£ ì‹ ê·œ ìƒí’ˆ ì•„ì´í—ˆë¸Œ ë§¤ì¹­ ({len(new_df)}ê°œ)")
                # outputs í´ë”ì— ì‹¤ì‹œê°„ ì €ì¥ë˜ë„ë¡ ê²½ë¡œ ì§€ì •
                match_output = os.path.join(outputs_dir, f"matched_new_{timestamp}.csv")
                matched_df = product_updater.match_iherb_products(new_df, output_path=match_output)
            else:
                print(f"\n3ï¸âƒ£ ì‹ ê·œ ìƒí’ˆ ì—†ìŒ")
                matched_df = pd.DataFrame()
            
            # 4. ìµœì¢… í†µí•©
            print(f"\n4ï¸âƒ£ ìµœì¢… ë°ì´í„° í†µí•©")
            final_df = data_processor.integrate_final_data(updated_base_df, matched_df)
            
        else:
            print(f"ğŸ“‹ ì´ˆê¸° ëª¨ë“œ (ì „ì²´ ë§¤ì¹­)")
            
            # 1. ì¿ íŒ¡ í¬ë¡¤ë§
            print(f"\n1ï¸âƒ£ ì¿ íŒ¡ ë°ì´í„° í¬ë¡¤ë§")
            crawled_df = product_updater.crawl_coupang_products(search_url)
            
            if len(crawled_df) == 0:
                return ""
                
            # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥
            crawled_df.to_csv(coupang_csv, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {coupang_csv}")
            
            # 2. ì „ì²´ ë§¤ì¹­ (outputs í´ë”ì— ì‹¤ì‹œê°„ ì €ì¥)
            print(f"\n2ï¸âƒ£ ì „ì²´ ìƒí’ˆ ì•„ì´í—ˆë¸Œ ë§¤ì¹­")
            match_output = os.path.join(outputs_dir, f"matched_all_{timestamp}.csv")
            final_df = product_updater.match_iherb_products(crawled_df, output_path=match_output)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_df.to_csv(final_csv, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ì™„ë£Œ: {final_csv} (ì´ {len(final_df)}ê°œ)")
        
        return final_csv
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        product_updater.close()


def resume_iherb_matching(coupang_csv_path: str) -> str:
    """
    ì•„ì´í—ˆë¸Œ ë§¤ì¹­ë§Œ ì¬ì‹œì‘í•˜ëŠ” í•¨ìˆ˜
    
    ì‚¬ìš©ë²•:
    python improved_updater_simple.py --resume outputs/coupang_crawled_20250924.csv
    """
    if not os.path.exists(coupang_csv_path):
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {coupang_csv_path}")
        return ""
    
    print(f"ğŸ”„ ì•„ì´í—ˆë¸Œ ë§¤ì¹­ ì¬ì‹œì‘")
    print(f"   ì¿ íŒ¡ ë°ì´í„°: {coupang_csv_path}")
    
    # ì¿ íŒ¡ ë°ì´í„° ë¡œë“œ
    crawled_df = pd.read_csv(coupang_csv_path, encoding='utf-8-sig')
    print(f"   ìƒí’ˆ ìˆ˜: {len(crawled_df)}ê°œ")
    
    # outputs í´ë”ì— ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    match_output = os.path.join("./outputs", f"resumed_matching_{timestamp}.csv")
    
    product_updater = ProductUpdater(enable_images=True)
    try:
        # ì•„ì´í—ˆë¸Œ ë§¤ì¹­ë§Œ ìˆ˜í–‰ (ìë™ ì¬ì‹œì‘ ì§€ì›)
        matched_df = product_updater.match_iherb_products(crawled_df, output_path=match_output)
        print(f"âœ… ë§¤ì¹­ ì™„ë£Œ: {match_output}")
        return match_output
    finally:
        product_updater.close()


def main():
    """ëª…ë ¹ì¤„ ì‹¤í–‰"""
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  ìƒˆë¡œìš´ ì‹¤í–‰: python improved_updater_simple.py <ì¿ íŒ¡_URL> [ê¸°ì¡´_CSV]")
        print("  ë§¤ì¹­ ì¬ì‹œì‘: python improved_updater_simple.py --resume <ì¿ íŒ¡_í¬ë¡¤ë§_CSV>")
        return 1
    
    if sys.argv[1] == "--resume":
        if len(sys.argv) < 3:
            print("âŒ ì¬ì‹œì‘í•  ì¿ íŒ¡ CSV íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return 1
        result = resume_iherb_matching(sys.argv[2])
    else:
        search_url = sys.argv[1]
        base_csv = sys.argv[2] if len(sys.argv) > 2 else None
        result = run_pipeline(search_url, base_csv)
    
    if result:
        print(f"\nğŸ‰ ì™„ë£Œ: {result}")
        return 0
    else:
        return 1


if __name__ == "__main__":
    sys.exit(main())