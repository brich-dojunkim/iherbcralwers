"""
í†µí•© íŒŒì´í”„ë¼ì¸ updater.py - ë£¨íŠ¸ í´ë” ìœ„ì¹˜
ì¿ íŒ¡/ì•„ì´í—ˆë¸Œ/ì—…ë°ì´í„° ëª¨ë“ˆì„ ì§ì ‘ í™œìš©í•˜ì—¬ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
"""

import pandas as pd
import sys
import os
import tempfile
from datetime import datetime

# ì„¸ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(__file__)
sys.path.insert(0, os.path.join(current_dir, 'coupang'))
sys.path.insert(0, os.path.join(current_dir, 'iherbscraper'))
sys.path.insert(0, os.path.join(current_dir, 'updater'))

# ëª¨ë“ˆ import
try:
    # ì¿ íŒ¡ ëª¨ë“ˆ
    from crawler import CoupangCrawlerMacOS
    from translator import GeminiCSVTranslator
    COUPANG_AVAILABLE = True
except ImportError as e:
    print(f"ì¿ íŒ¡ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    COUPANG_AVAILABLE = False

try:
    # ì•„ì´í—ˆë¸Œ ëª¨ë“ˆ
    from main import EnglishIHerbScraper
    from config import Config
    IHERB_AVAILABLE = True
except ImportError as e:
    print(f"ì•„ì´í—ˆë¸Œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    IHERB_AVAILABLE = False

try:
    # ì—…ë°ì´í„° ëª¨ë“ˆ (ë°ì´í„° ì²˜ë¦¬ ë¡œì§ë§Œ)
    from data_processor import DataProcessor
    UPDATER_AVAILABLE = True
except ImportError as e:
    print(f"ì—…ë°ì´í„° ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    UPDATER_AVAILABLE = False


def run_integrated_pipeline(search_url: str, base_csv: str = None, output_file: str = None) -> str:
    """
    í†µí•© íŒŒì´í”„ë¼ì¸ - 3ê°œ ëª¨ë“ˆ ì§ì ‘ í™œìš©
    ì´ˆê¸°ê°’ í™•ë³´ì™€ ì—…ë°ì´íŠ¸ë¥¼ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬
    
    Args:
        search_url: ì¿ íŒ¡ ê²€ìƒ‰ URL
        base_csv: ê¸°ì¡´ CSV (ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ ëª¨ë“œ)
        output_file: ì¶œë ¥ íŒŒì¼ëª…
        
    Returns:
        ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ
    """
    # í•„ìˆ˜ ëª¨ë“ˆ ì²´í¬
    if not all([COUPANG_AVAILABLE, IHERB_AVAILABLE, UPDATER_AVAILABLE]):
        print("í•„ìš”í•œ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return ""
    
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"integrated_results_{timestamp}.csv"
    
    is_update_mode = base_csv and os.path.exists(base_csv)
    
    print(f"ğŸš€ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    print(f"ğŸ“ ì¿ íŒ¡ URL: {search_url[:50]}...")
    print(f"ğŸ“„ ëª¨ë“œ: {'ì—…ë°ì´íŠ¸' if is_update_mode else 'ì´ˆê¸°ê°’'}")
    if is_update_mode:
        print(f"ğŸ“„ ê¸°ì¡´ íŒŒì¼: {base_csv}")
    print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {output_file}")
    
    coupang_crawler = None
    iherb_scraper = None
    
    try:
        # ========================
        # 1ë‹¨ê³„: ì¿ íŒ¡ í¬ë¡¤ë§
        # ========================
        print(f"\n1ï¸âƒ£ ì¿ íŒ¡ í¬ë¡¤ë§ (ì§ì ‘ ëª¨ë“ˆ í™œìš©)")
        coupang_crawler = CoupangCrawlerMacOS(
            headless=False, 
            download_images=True,
            delay_range=(3, 6)
        )
        
        products = coupang_crawler.crawl_all_pages(search_url)
        
        if not products:
            print("âŒ í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ")
            return ""
        
        coupang_df = pd.DataFrame(products)
        print(f"âœ… ì¿ íŒ¡ í¬ë¡¤ë§ ì™„ë£Œ: {len(coupang_df)}ê°œ ìƒí’ˆ")
        
        # ========================
        # 2ë‹¨ê³„: ë²ˆì—­
        # ========================
        print(f"\n2ï¸âƒ£ ë²ˆì—­ (ì¿ íŒ¡ ëª¨ë“ˆ)")
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8-sig') as f:
            temp_coupang_path = f.name
            coupang_df.to_csv(temp_coupang_path, index=False, encoding='utf-8-sig')
        
        # ë²ˆì—­ ìˆ˜í–‰
        translator = GeminiCSVTranslator(Config.GEMINI_API_KEY)
        translated_df = translator.translate_csv(
            input_file=temp_coupang_path,
            output_file=temp_coupang_path,
            column_name='product_name',
            batch_size=10
        )
        print(f"âœ… ë²ˆì—­ ì™„ë£Œ")
        
        # ========================
        # 3ë‹¨ê³„: ì•„ì´í—ˆë¸Œ ë§¤ì¹­
        # ========================
        print(f"\n3ï¸âƒ£ ì•„ì´í—ˆë¸Œ ë§¤ì¹­ (ì§ì ‘ ëª¨ë“ˆ í™œìš©)")
        
        # ì„ì‹œ ë§¤ì¹­ ê²°ê³¼ íŒŒì¼
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8-sig') as f:
            temp_matched_path = f.name
        
        # ì•„ì´í—ˆë¸Œ ìŠ¤í¬ë˜í¼ ì‹¤í–‰
        iherb_scraper = EnglishIHerbScraper(
            headless=False,
            delay_range=(2, 4),
            max_products_to_compare=4
        )
        
        matched_csv = iherb_scraper.process_products_complete(
            csv_file_path=temp_coupang_path,
            output_file_path=temp_matched_path,
            limit=None,
            start_from=None
        )
        
        # ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ
        if os.path.exists(temp_matched_path):
            matched_df = pd.read_csv(temp_matched_path, encoding='utf-8-sig')
            success_count = len(matched_df[matched_df.get('status', '') == 'success'])
            print(f"âœ… ì•„ì´í—ˆë¸Œ ë§¤ì¹­ ì™„ë£Œ: {success_count}/{len(matched_df)}ê°œ ì„±ê³µ")
        else:
            print("âŒ ë§¤ì¹­ ê²°ê³¼ íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
            return ""
        
        # ========================
        # 4ë‹¨ê³„: ë°ì´í„° í†µí•© (ì—…ë°ì´í„° ëª¨ë“ˆ í™œìš©)
        # ========================
        print(f"\n4ï¸âƒ£ ë°ì´í„° í†µí•© (ì—…ë°ì´í„° ëª¨ë“ˆ)")
        
        if is_update_mode:
            # ì—…ë°ì´íŠ¸ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„°ì™€ í†µí•©
            processor = DataProcessor()
            final_df = _integrate_with_existing_data(matched_df, base_csv, processor)
        else:
            # ì´ˆê¸°ê°’ ëª¨ë“œ: ê·¸ëŒ€ë¡œ ì‚¬ìš©
            final_df = matched_df
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_file in [temp_coupang_path, temp_matched_path]:
            try:
                os.unlink(temp_file)
            except:
                pass
        
        # ìµœì¢… ê²°ê³¼
        final_success = len(final_df[final_df.get('status', '') == 'success'])
        print(f"âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {final_success}/{len(final_df)}ê°œ ìµœì¢… ë§¤ì¹­")
        
        return output_file
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return ""
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if coupang_crawler:
            coupang_crawler.close()
        if iherb_scraper:
            iherb_scraper.close()


def _integrate_with_existing_data(new_df: pd.DataFrame, base_csv: str, processor) -> pd.DataFrame:
    """ê¸°ì¡´ ë°ì´í„°ì™€ ì‹ ê·œ ë°ì´í„° í†µí•© - ì—…ë°ì´í„° ëª¨ë“ˆ ë¡œì§ í™œìš©"""
    try:
        existing_df = pd.read_csv(base_csv, encoding='utf-8-sig')
        print(f"   ê¸°ì¡´ ë°ì´í„°: {len(existing_df)}ê°œ")
        
        # ìƒí’ˆ ID ê¸°ì¤€ ë¹„êµ
        existing_ids = set(existing_df.get('coupang_product_id', []).astype(str))
        new_ids = set(new_df.get('coupang_product_id', []).astype(str))
        
        truly_new = new_ids - existing_ids
        continuing = new_ids & existing_ids
        missing = existing_ids - new_ids
        
        print(f"   ì‹ ê·œ: {len(truly_new)}, ê³„ì†: {len(continuing)}, ì‚¬ë¼ì§: {len(missing)}")
        
        # ì—…ë°ì´í„° ëª¨ë“ˆì˜ ë°ì´í„° í†µí•© ë¡œì§ í™œìš©
        # (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë²„ì „ìœ¼ë¡œ êµ¬í˜„, í•„ìš”ì‹œ processorì˜ ë©”ì„œë“œ í™œìš© ê°€ëŠ¥)
        
        # ê¸°ì¡´ ë°ì´í„° ë³µì‚¬
        final_df = existing_df.copy()
        
        # ê³„ì† íŒë§¤ë˜ëŠ” ìƒí’ˆì˜ ê°€ê²© ì—…ë°ì´íŠ¸
        for idx, row in final_df.iterrows():
            product_id = str(row.get('coupang_product_id', ''))
            if product_id in continuing:
                new_row = new_df[new_df['coupang_product_id'].astype(str) == product_id].iloc[0]
                final_df.at[idx, 'coupang_current_price_krw'] = new_row.get('coupang_current_price_krw', '')
                final_df.at[idx, 'coupang_discount_rate'] = new_row.get('coupang_discount_rate', '')
        
        # ì‹ ê·œ ìƒí’ˆ ì¶”ê°€
        if truly_new:
            new_products = new_df[new_df['coupang_product_id'].astype(str).isin(truly_new)]
            final_df = pd.concat([final_df, new_products], ignore_index=True)
        
        return final_df
        
    except Exception as e:
        print(f"   ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}, ì‹ ê·œ ë°ì´í„°ë§Œ ì‚¬ìš©")
        return new_df


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    if len(sys.argv) < 2:
        print("ğŸ¯ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ê¸°")
        print("ì´ˆê¸°ê°’: python updater.py <ì¿ íŒ¡_URL>")
        print("ì—…ë°ì´íŠ¸: python updater.py <ì¿ íŒ¡_URL> <ê¸°ì¡´_CSV>")
        print()
        print("ì˜ˆì‹œ:")
        print("  python updater.py 'https://www.coupang.com/np/search?q=thorne'")
        print("  python updater.py 'https://www.coupang.com/np/search?q=thorne' results.csv")
        return
    
    search_url = sys.argv[1]
    base_csv = sys.argv[2] if len(sys.argv) > 2 else None
    
    result = run_integrated_pipeline(search_url, base_csv)
    
    if result:
        print(f"\nğŸ‰ ì™„ë£Œ: {result}")
    else:
        print(f"\nğŸ’¥ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()