"""
YouTube ì±„ë„ ë™ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ë° ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸
Groq Whisper API(ì´ˆê³ ì† STT) + Gemini(í…ìŠ¤íŠ¸ ìš”ì•½)
"""

import json
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import time
from datetime import datetime, timedelta
from typing import List, Dict

import yt_dlp
from google import genai
from google.genai.types import HttpOptions
from groq import Groq  # Groq API ì¶”ê°€

import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
from openpyxl.utils import get_column_letter

try:
    from faster_whisper import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("âš ï¸  faster-whisper ë¯¸ì„¤ì¹˜. Groqë§Œ ì‚¬ìš© ê°€ëŠ¥.")
    print("   ì„¤ì¹˜: pip install faster-whisper")

# YouTube ì±„ë„ ëª©ë¡
CHANNELS = [
    "https://www.youtube.com/@yakstory119",
    "https://www.youtube.com/@Ojingeryaksa",
    "https://www.youtube.com/@ë¦¬í‹€ì•½ì‚¬",
    "https://www.youtube.com/@ì•½ì‚¬ë©”ë””ìŠ¨ë§¨",
    "https://www.youtube.com/@ì–‘ê³¼ì",
    "https://www.youtube.com/@HongSee_yaksa"
]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Rate Limit ì¶”ì  í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class RateLimitTracker:
    """
    Groq Whisper APIì˜ ASH (Audio Seconds per Hour) ì œí•œ ì¶”ì 
    ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ê³¼ê±° 1ì‹œê°„ ë™ì•ˆì˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬ëŸ‰ ì¶”ì 
    """
    def __init__(self, limit_ash: int = 7000):  # 7200ì´ˆ ì¤‘ 200ì´ˆëŠ” ë²„í¼
        self.limit_ash = limit_ash
        self.requests = []  # [(timestamp, duration), ...]
    
    def _clean_old_requests(self):
        """1ì‹œê°„ ì´ìƒ ëœ ìš”ì²­ ì œê±°"""
        now = time.time()
        self.requests = [(t, d) for t, d in self.requests if now - t < 3600]
    
    def get_current_ash(self) -> int:
        """í˜„ì¬ 1ì‹œê°„ ìœˆë„ìš°ì˜ ëˆ„ì  ì˜¤ë””ì˜¤ ì´ˆ"""
        self._clean_old_requests()
        return sum(d for _, d in self.requests)
    
    def can_process(self, duration: int) -> bool:
        """ì§€ì •ëœ durationì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸"""
        current = self.get_current_ash()
        return current + duration <= self.limit_ash
    
    def wait_time_needed(self, duration: int) -> float:
        """
        durationì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        ê°€ì¥ ì˜¤ë˜ëœ ìš”ì²­ì´ ìœˆë„ìš°ì—ì„œ ë¹ ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
        """
        self._clean_old_requests()
        current = self.get_current_ash()
        
        if current + duration <= self.limit_ash:
            return 0
        
        # ëª©í‘œ: current_ashê°€ limit - duration ì´í•˜ë¡œ ë–¨ì–´ì§ˆ ë•Œê¹Œì§€
        target = self.limit_ash - duration
        
        if not self.requests:
            return 0
        
        # ìš”ì²­ë“¤ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì œê±°í•˜ë©´ì„œ ê³„ì‚°
        accumulated = current
        now = time.time()
        
        for timestamp, req_duration in sorted(self.requests, key=lambda x: x[0]):
            accumulated -= req_duration
            if accumulated <= target:
                # ì´ ìš”ì²­ì´ ë§Œë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                wait = (timestamp + 3600) - now
                return max(wait, 0) + 10  # 10ì´ˆ ë²„í¼
        
        # ëª¨ë“  ìš”ì²­ì´ ë§Œë£Œë˜ì–´ì•¼ í•¨
        oldest_time = self.requests[0][0]
        return max((oldest_time + 3600) - now, 0) + 10
    
    def record(self, duration: int):
        """ì²˜ë¦¬í•œ ì˜¤ë””ì˜¤ duration ê¸°ë¡"""
        self.requests.append((time.time(), duration))
    
    def get_stats(self) -> Dict:
        """í˜„ì¬ ìƒíƒœ í†µê³„"""
        current = self.get_current_ash()
        return {
            'current_ash': current,
            'limit_ash': self.limit_ash,
            'remaining_ash': self.limit_ash - current,
            'usage_percent': (current / self.limit_ash) * 100,
            'requests_in_window': len(self.requests)
        }


# ì¤‘ê°„ ì €ì¥ íŒŒì¼ ê²½ë¡œ
CHECKPOINT_DIR = "checkpoints"
VIDEOS_CHECKPOINT = os.path.join(CHECKPOINT_DIR, "collected_videos.json")
SUMMARIES_CHECKPOINT = os.path.join(CHECKPOINT_DIR, "summaries.json")
TRANSCRIPTS_CHECKPOINT = os.path.join(CHECKPOINT_DIR, "transcripts.csv")

# ì˜¤ë””ì˜¤ ì €ì¥ ë””ë ‰í† ë¦¬
AUDIO_DIR = "audio"

# Whisper ëª¨ë¸ ìºì‹œ (ë¡œì»¬ í´ë°±ìš©)
_WHISPER_MODEL = None


def get_whisper_model(model_size: str = "small"):
    """Whisper ë¡œì»¬ ëª¨ë¸ ë¡œë“œ (í´ë°±ìš©)"""
    global _WHISPER_MODEL
    if _WHISPER_MODEL is None:
        if not WHISPER_AVAILABLE:
            raise Exception("faster-whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print(f"   ğŸ§  Whisper ë¡œì»¬ ëª¨ë¸ ë¡œë”©... (size={model_size})")
        _WHISPER_MODEL = WhisperModel(model_size, device="cpu", compute_type="int8")
        print(f"   âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return _WHISPER_MODEL


def transcribe_with_local_whisper(audio_path: str) -> str:
    """
    ë¡œì»¬ Whisper STT (Groq í´ë°±ìš©)
    ëŠë¦¬ì§€ë§Œ ì œí•œ ì—†ìŒ
    """
    print(f"   ğŸ”„ ë¡œì»¬ Whisper STT ì‚¬ìš© (Groq í´ë°±)")
    
    model = get_whisper_model(model_size="small")
    
    file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
    print(f"   ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")
    
    start_time = time.time()
    segments, info = model.transcribe(
        audio_path,
        language="ko",
        beam_size=5
    )
    
    print(f"   â³ ìŒì„± ê¸¸ì´: {info.duration:.0f}ì´ˆ")
    
    texts = []
    for seg in segments:
        texts.append(seg.text.strip())
    
    transcript = " ".join(texts)
    elapsed = time.time() - start_time
    
    print(f"   âœ… ë¡œì»¬ STT ì™„ë£Œ ({len(transcript)}ì, {elapsed:.1f}ì´ˆ ì†Œìš”)")
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
    try:
        os.remove(audio_path)
        print(f"   ğŸ—‘ï¸  ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ")
    except Exception as e:
        print(f"   âš ï¸  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
    return transcript


def ensure_checkpoint_dir():
    if not os.path.exists(CHECKPOINT_DIR):
        os.makedirs(CHECKPOINT_DIR)


def ensure_audio_dir():
    if not os.path.exists(AUDIO_DIR):
        os.makedirs(AUDIO_DIR)


def save_videos_checkpoint(videos: List[Dict]):
    ensure_checkpoint_dir()
    with open(VIDEOS_CHECKPOINT, 'w', encoding='utf-8') as f:
        json.dump(videos, f, ensure_ascii=False, indent=2)
    print(f"   ğŸ’¾ ë™ì˜ìƒ ì •ë³´ ì €ì¥: {len(videos)}ê°œ")


def load_videos_checkpoint() -> List[Dict]:
    if os.path.exists(VIDEOS_CHECKPOINT):
        with open(VIDEOS_CHECKPOINT, 'r', encoding='utf-8') as f:
            videos = json.load(f)
        print(f"   ğŸ“‚ ê¸°ì¡´ ë™ì˜ìƒ ë¡œë“œ: {len(videos)}ê°œ")
        return videos
    return []


def save_summaries_checkpoint(summaries: List[Dict]):
    ensure_checkpoint_dir()
    with open(SUMMARIES_CHECKPOINT, 'w', encoding='utf-8') as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)
    print(f"   ğŸ’¾ ìš”ì•½ ê²°ê³¼ ì €ì¥: {len(summaries)}ê°œ")


def load_summaries_checkpoint() -> List[Dict]:
    if os.path.exists(SUMMARIES_CHECKPOINT):
        with open(SUMMARIES_CHECKPOINT, 'r', encoding='utf-8') as f:
            summaries = json.load(f)
        print(f"   ğŸ“‚ ê¸°ì¡´ ìš”ì•½ ë¡œë“œ: {len(summaries)}ê°œ")
        return summaries
    return []


def save_transcripts_checkpoint(transcripts: Dict[str, str]):
    """transcriptë¥¼ CSVë¡œ ì €ì¥"""
    ensure_checkpoint_dir()
    data = [{'video_id': vid, 'transcript': text, 'length': len(text)} 
            for vid, text in transcripts.items()]
    df = pd.DataFrame(data)
    df.to_csv(TRANSCRIPTS_CHECKPOINT, index=False, encoding='utf-8-sig')
    print(f"   ğŸ’¾ transcript ì €ì¥: {len(transcripts)}ê°œ")


def load_transcripts_checkpoint() -> Dict[str, str]:
    """ì €ì¥ëœ transcript ë¶ˆëŸ¬ì˜¤ê¸°"""
    if os.path.exists(TRANSCRIPTS_CHECKPOINT):
        df = pd.read_csv(TRANSCRIPTS_CHECKPOINT, encoding='utf-8-sig')
        transcripts = dict(zip(df['video_id'], df['transcript']))
        print(f"   ğŸ“‚ ê¸°ì¡´ transcript ë¡œë“œ: {len(transcripts)}ê°œ")
        return transcripts
    return {}


def clear_checkpoints():
    files_to_remove = [VIDEOS_CHECKPOINT, SUMMARIES_CHECKPOINT, TRANSCRIPTS_CHECKPOINT]
    for filepath in files_to_remove:
        if os.path.exists(filepath):
            os.remove(filepath)
    print(f"   ğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì™„ë£Œ")


def get_channel_videos(channel_url: str, days_back: int = 30) -> List[Dict]:
    """ì±„ë„ì˜ ìµœê·¼ ë™ì˜ìƒ ì •ë³´ ìˆ˜ì§‘"""
    cutoff_date = datetime.now() - timedelta(days=days_back)
    
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': False,
        'playlistend': 50,
        'skip_download': True,
        'ignoreerrors': True,
        'noprogress': True,
    }
    
    videos = []
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            print(f"   â³ ì±„ë„ ì •ë³´ ë¡œë”© ì¤‘...")
            result = ydl.extract_info(f"{channel_url}/videos", download=False)
            
            if 'entries' in result:
                channel_name = result.get('channel', 'Unknown')
                print(f"   âœ… ì±„ë„: {channel_name}")
                
                entries = [e for e in result['entries'] if e]
                collected = 0
                
                for entry in entries:
                    try:
                        if entry.get('availability') == 'subscriber_only':
                            continue
                        
                        upload_date = datetime.strptime(entry.get('upload_date', '20000101'), '%Y%m%d')
                        
                        if upload_date >= cutoff_date:
                            video_id = entry.get('id', '')
                            videos.append({
                                'channel': channel_name,
                                'channel_url': channel_url,
                                'title': entry.get('title', ''),
                                'video_id': video_id,
                                'url': f"https://www.youtube.com/watch?v={video_id}",
                                'upload_date': entry.get('upload_date', ''),
                                'view_count': entry.get('view_count', 0),
                                'duration': entry.get('duration', 0),
                                'description': entry.get('description', '')[:500],
                            })
                            collected += 1
                    except:
                        continue
                
                print(f"   âœ… ìˆ˜ì§‘: {collected}ê°œ")
    
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {str(e)[:100]}")
    
    return videos


def download_audio_for_video(video_info: Dict) -> str:
    """ì˜¤ë””ì˜¤ë§Œ ë‹¤ìš´ë¡œë“œ"""
    ensure_audio_dir()
    video_url = video_info["url"]
    video_id = video_info["video_id"]
    
    ydl_opts = {
        "quiet": True,
        "no_warnings": True,
        "format": "bestaudio/best",
        "outtmpl": os.path.join(AUDIO_DIR, f"{video_id}.%(ext)s"),
        "noprogress": True,  # ì§„í–‰ë°” ìˆ¨ê¸°ê¸°
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        print(f"   ğŸ§ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        info = ydl.extract_info(video_url, download=True)
        filename = ydl.prepare_filename(info)
    
    return filename


def transcribe_with_groq(audio_path: str) -> str:
    """
    Groq Whisper APIë¡œ ì´ˆê³ ì† STT
    - íŒŒì¼ í¬ê¸° ì œí•œ: 25MB
    - Rate Limit ë°œìƒ ì‹œ retry-after ê¸°ë°˜ ëŒ€ê¸°
    """
    print(f"   ğŸ“ Groq Whisper API í˜¸ì¶œ ì¤‘...")
    
    groq_api_key = os.getenv('GROQ_API_KEY')
    if not groq_api_key:
        raise Exception("GROQ_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
    print(f"   ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")
    
    # 25MB ì´ˆê³¼ ì‹œ ì—ëŸ¬
    if file_size_mb > 25:
        raise Exception(f"íŒŒì¼ í¬ê¸° ì´ˆê³¼ ({file_size_mb:.1f}MB > 25MB). ì˜ìƒì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.")
    
    client = Groq(api_key=groq_api_key)
    
    # Rate Limit ëŒ€ì‘: ìµœëŒ€ 3íšŒ ì¬ì‹œë„
    max_retries = 3
    for attempt in range(max_retries):
        try:
            with open(audio_path, "rb") as audio_file:
                start_time = time.time()
                transcription = client.audio.transcriptions.create(
                    file=audio_file,
                    model="whisper-large-v3-turbo",
                    language="ko",
                    response_format="text"
                )
                elapsed = time.time() - start_time
            
            transcript = transcription
            print(f"   âœ… STT ì™„ë£Œ ({len(transcript)}ì, {elapsed:.1f}ì´ˆ ì†Œìš”)")
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
            try:
                os.remove(audio_path)
                print(f"   ğŸ—‘ï¸  ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ")
            except Exception as e:
                print(f"   âš ï¸  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            return transcript
            
        except Exception as e:
            error_str = str(e)
            
            # Rate Limit ì—ëŸ¬ì¸ ê²½ìš°
            if "429" in error_str or "rate limit" in error_str.lower():
                # ì—ëŸ¬ ë©”ì‹œì§€ ì „ì²´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print(f"   âš ï¸  Rate Limit ë°œìƒ")
                
                # retry ì‹œê°„ íŒŒì‹±
                wait_time = parse_retry_time(error_str)
                
                if attempt < max_retries - 1:
                    print(f"   â³ {wait_time}ì´ˆ ({wait_time/60:.1f}ë¶„) ëŒ€ê¸° í›„ ì¬ì‹œë„... ({attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    raise Exception(f"Rate Limit ì´ˆê³¼ ({max_retries}íšŒ ì¬ì‹œë„ ì‹¤íŒ¨). ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            else:
                # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ë°”ë¡œ raise
                raise


def parse_retry_time(error_msg: str) -> int:
    """
    Groq ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ì •í™•í•œ ëŒ€ê¸° ì‹œê°„ íŒŒì‹±
    ì‹¤ì œ í˜•ì‹: "Please try again in 7m36.5s"
    """
    import re
    
    # ëª¨ë“  ì‹œê°„ íŒ¨í„´ ì°¾ê¸° (ë¶„ê³¼ ì´ˆ ì¡°í•©)
    # 7m36.5s, 52m3s, 7m36s ë“± ëª¨ë‘ ë§¤ì¹­
    pattern = r'(\d+)m([\d.]+)s'
    match = re.search(pattern, error_msg)
    
    if match:
        minutes = int(match.group(1))
        seconds = float(match.group(2))
        total = minutes * 60 + int(seconds) + 10  # 10ì´ˆ ë²„í¼
        print(f"   ğŸ” ëŒ€ê¸° ì‹œê°„ íŒŒì‹±: {minutes}ë¶„ {seconds:.1f}ì´ˆ â†’ {total}ì´ˆ ëŒ€ê¸°")
        return total
    
    # ì´ˆë§Œ ìˆëŠ” ê²½ìš°: "45s"
    pattern2 = r'(\d+)s'
    match = re.search(pattern2, error_msg)
    if match:
        seconds = int(match.group(1)) + 10
        print(f"   ğŸ” ëŒ€ê¸° ì‹œê°„ íŒŒì‹±: {seconds-10}ì´ˆ â†’ {seconds}ì´ˆ ëŒ€ê¸°")
        return seconds
    
    # íŒŒì‹± ì‹¤íŒ¨ (ë°œìƒí•˜ë©´ ì•ˆ ë¨)
    print(f"   âš ï¸  íŒŒì‹± ì‹¤íŒ¨! ê¸°ë³¸ 10ë¶„ ëŒ€ê¸°")
    print(f"   ğŸ“„ ì—ëŸ¬ ë©”ì‹œì§€: {error_msg[:200]}")
    return 600


def summarize_video_with_gemini(video_info: Dict, transcript: str) -> str:
    """Geminië¡œ í…ìŠ¤íŠ¸ ìš”ì•½"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        return "ìš”ì•½ ì‹¤íŒ¨: GEMINI_API_KEY ë¯¸ì„¤ì •"
    
    client = genai.Client(
        api_key=api_key,
        http_options=HttpOptions(api_version="v1")
    )
    
    prompt = f"""ì•½ì‚¬ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ - ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì •ë³´ ì¶”ì¶œ

ì œëª©: {video_info['title']}
ì±„ë„: {video_info['channel']}

[ë°›ì•„ì“°ê¸° ë‚´ìš©]
{transcript}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½:

**1. í•µì‹¬ ì£¼ì œ** (1ë¬¸ì¥)

**2. ì œí’ˆ/ì„±ë¶„ ì •ë³´**
ğŸ“¦ ì œí’ˆëª…: [ë¸Œëœë“œ+ì œí’ˆëª…] ë˜ëŠ” "ì–¸ê¸‰ ì—†ìŒ"
ğŸ§ª ì„±ë¶„ëª…: [êµ¬ì²´ì  ì„±ë¶„] ë˜ëŠ” "ì–¸ê¸‰ ì—†ìŒ"  
ğŸ’Š ìš©ë„: [ê±´ê°• ëª©ì ]
ğŸ‘¨â€âš•ï¸ ì•½ì‚¬ ì˜ê²¬: [ì¶”ì²œ/ì£¼ì˜/ì¤‘ë¦½]

**3. ì£¼ìš” ë‚´ìš©** (3-5ê°œ í¬ì¸íŠ¸)

**4. íŠ¸ë Œë“œ ì‹œì‚¬ì ** (1-2ë¬¸ì¥)
"""
    
    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        return response.text
    except Exception as e:
        if "429" in str(e) or "quota" in str(e).lower():
            return "ìš”ì•½ ì‹¤íŒ¨: ì¿¼í„° ì´ˆê³¼"
        return f"ìš”ì•½ ì‹¤íŒ¨: {str(e)[:200]}"


def save_to_excel(all_videos: List[Dict], summaries: List[Dict], output_filename: str):
    """ì—‘ì…€ ì €ì¥"""
    print(f"\nğŸ“Š ì—‘ì…€ ìƒì„± ì¤‘...")
    
    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
        # 1. ì „ì²´ ë™ì˜ìƒ
        df_all = pd.DataFrame(all_videos)
        df_all['ì—…ë¡œë“œì¼'] = pd.to_datetime(df_all['upload_date'], format='%Y%m%d').dt.strftime('%Y-%m-%d')
        
        df_display = df_all[['channel', 'title', 'ì—…ë¡œë“œì¼', 'view_count', 'duration', 'url']].copy()
        df_display.columns = ['ì±„ë„', 'ì œëª©', 'ì—…ë¡œë“œì¼', 'ì¡°íšŒìˆ˜', 'ê¸¸ì´(ì´ˆ)', 'URL']
        df_display['ì¡°íšŒìˆ˜'] = df_display['ì¡°íšŒìˆ˜'].apply(lambda x: f"{x:,}")
        df_display.to_excel(writer, sheet_name='ì „ì²´ ë™ì˜ìƒ ëª©ë¡', index=False)
        
        # 2. AI ìš”ì•½
        if summaries:
            summary_data = []
            for item in summaries:
                v = item['video_info']
                summary_data.append({
                    'ì±„ë„': v['channel'],
                    'ì œëª©': v['title'],
                    'ì—…ë¡œë“œì¼': datetime.strptime(v['upload_date'], '%Y%m%d').strftime('%Y-%m-%d'),
                    'ì¡°íšŒìˆ˜': f"{v['view_count']:,}",
                    'URL': v['url'],
                    'AI ìš”ì•½': item['summary']
                })
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='AI ìš”ì•½ ê²°ê³¼', index=False)
        
        # 3. ì±„ë„ í†µê³„
        stats = df_all.groupby('channel').agg({
            'title': 'count',
            'view_count': ['sum', 'mean', 'max'],
            'duration': 'sum'
        }).reset_index()
        stats.columns = ['ì±„ë„', 'ë™ì˜ìƒ ìˆ˜', 'ì´ ì¡°íšŒìˆ˜', 'í‰ê·  ì¡°íšŒìˆ˜', 'ìµœëŒ€ ì¡°íšŒìˆ˜', 'ì´ ê¸¸ì´(ì´ˆ)']
        for col in ['ì´ ì¡°íšŒìˆ˜', 'í‰ê·  ì¡°íšŒìˆ˜', 'ìµœëŒ€ ì¡°íšŒìˆ˜']:
            stats[col] = stats[col].apply(lambda x: f"{int(x):,}")
        stats['ì´ ê¸¸ì´(ë¶„)'] = (stats['ì´ ê¸¸ì´(ì´ˆ)'].astype(int) / 60).round(1)
        stats = stats.drop('ì´ ê¸¸ì´(ì´ˆ)', axis=1)
        stats.to_excel(writer, sheet_name='ì±„ë„ë³„ í†µê³„', index=False)
    
    print(f"   âœ… ì—‘ì…€ ìƒì„± ì™„ë£Œ")


def main():
    start_time = datetime.now().timestamp()
    
    print("=" * 80)
    print("ğŸ¬ YouTube ì•½ì‚¬ ì±„ë„ ë¶„ì„ ì‹œìŠ¤í…œ (Groq Whisper + Gemini)")
    print("=" * 80)
    
    days_back = 30
    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days_back}ì¼")
    print(f"ğŸ“º ëŒ€ìƒ ì±„ë„: {len(CHANNELS)}ê°œ")
    
    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    existing_videos = load_videos_checkpoint()
    existing_summaries = load_summaries_checkpoint()
    existing_transcripts = load_transcripts_checkpoint()
    
    if existing_videos or existing_summaries:
        print(f"\nâš ï¸  ì´ì „ ì‘ì—… ë°œê²¬")
        print(f"   ë™ì˜ìƒ: {len(existing_videos)}ê°œ")
        print(f"   ìš”ì•½: {len(existing_summaries)}ê°œ")
        print(f"   transcript: {len(existing_transcripts)}ê°œ")
        response = input("ì´ì–´ì„œ ì§„í–‰? (y/n): ").lower()
        if response == 'y':
            all_videos = existing_videos
            summaries = existing_summaries
            transcripts = existing_transcripts
            skip_collection = True
        else:
            clear_checkpoints()
            all_videos = []
            summaries = []
            transcripts = {}
            skip_collection = False
    else:
        all_videos = []
        summaries = []
        transcripts = {}
        skip_collection = False
    
    # 1ë‹¨ê³„: ë™ì˜ìƒ ìˆ˜ì§‘
    if not skip_collection:
        print("\n" + "=" * 80)
        print("ğŸ” 1ë‹¨ê³„: ë™ì˜ìƒ ì •ë³´ ìˆ˜ì§‘")
        print("=" * 80)
        
        for idx, channel_url in enumerate(CHANNELS, 1):
            print(f"\n[{idx}/{len(CHANNELS)}] {channel_url.split('@')[1]}")
            videos = get_channel_videos(channel_url, days_back)
            all_videos.extend(videos)
            save_videos_checkpoint(all_videos)
        
        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(all_videos)}ê°œ")
        all_videos.sort(key=lambda x: x['upload_date'], reverse=True)
    
    # 2ë‹¨ê³„: STT + ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ¤– 2ë‹¨ê³„: STT + AI ìš”ì•½")
    print("=" * 80)
    
    summarized_ids = {s['video_info']['video_id'] for s in summaries}
    videos_to_summarize = [v for v in all_videos if v['video_id'] not in summarized_ids]
    
    print(f"\nì™„ë£Œ: {len(summaries)}, ëŒ€ê¸°: {len(videos_to_summarize)}")
    
    if videos_to_summarize:
        # ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
        total_audio_seconds = sum(v['duration'] for v in videos_to_summarize)
        estimated_hours = total_audio_seconds / 3600
        
        print(f"â±ï¸  ì´ ì˜¤ë””ì˜¤ ê¸¸ì´: {total_audio_seconds:,}ì´ˆ ({estimated_hours:.1f}ì‹œê°„)")
        print(f"ğŸ”§ Groq Whisper API + Gemini")
        print(f"ğŸ“Š ASH ì œí•œ: ì‹œê°„ë‹¹ 7,200ì´ˆ")
        
        # Rate Limit Tracker ì´ˆê¸°í™”
        rate_tracker = RateLimitTracker(limit_ash=7000)
        
        for i, video in enumerate(videos_to_summarize, 1):
            video_id = video['video_id']
            duration = video['duration']
            
            # all_videosì—ì„œì˜ ì‹¤ì œ ì¸ë±ìŠ¤ ì°¾ê¸°
            actual_index = next((idx for idx, v in enumerate(all_videos, 1) if v['video_id'] == video_id), i)
            
            print(f"\n[{actual_index}/{len(all_videos)}] {video['title'][:50]}")
            print(f"   â±ï¸  ì˜ìƒ ê¸¸ì´: {duration}ì´ˆ ({duration/60:.1f}ë¶„)")
            
            # Rate Limit ì²´í¬ ë° ëŒ€ê¸°
            if not rate_tracker.can_process(duration):
                wait_time = rate_tracker.wait_time_needed(duration)
                stats = rate_tracker.get_stats()
                
                print(f"   âš ï¸  ASH ì œí•œ ê·¼ì ‘ ({stats['current_ash']}/{stats['limit_ash']}ì´ˆ)")
                print(f"   â³ {wait_time}ì´ˆ ({wait_time/60:.1f}ë¶„) ëŒ€ê¸° ì¤‘...")
                
                time.sleep(wait_time)
                print(f"   âœ… ëŒ€ê¸° ì™„ë£Œ")
            
            try:
                # STT: ê¸°ì¡´ transcript ìˆìœ¼ë©´ ì¬ì‚¬ìš©
                if video_id in transcripts:
                    print(f"   â™»ï¸  ê¸°ì¡´ transcript ì¬ì‚¬ìš©")
                    transcript = transcripts[video_id]
                else:
                    audio_path = download_audio_for_video(video)
                    transcript = transcribe_with_groq(audio_path)
                    transcripts[video_id] = transcript
                    save_transcripts_checkpoint(transcripts)
                    
                    # STT ì„±ê³µ ì‹œ duration ê¸°ë¡
                    rate_tracker.record(duration)
                
                # ìš”ì•½
                summary = summarize_video_with_gemini(video, transcript)
                
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {str(e)[:200]}")
                summary = f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:200]}"
            
            summaries.append({
                'video_info': video,
                'summary': summary,
                'processed_at': datetime.now().isoformat()
            })
            
            save_summaries_checkpoint(summaries)
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            stats = rate_tracker.get_stats()
            print(f"   ğŸ“Š ASH: {stats['current_ash']}/{stats['limit_ash']}ì´ˆ ({stats['usage_percent']:.1f}%)")
            
            # ì˜ìƒ ê°„ ì§§ì€ ëŒ€ê¸°
            if i < len(videos_to_summarize):
                time.sleep(2)
    
    # ê²°ê³¼ ì €ì¥
    excel_file = f'youtube_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx'
    save_to_excel(all_videos, summaries, excel_file)
    
    print(f"\nâœ… ì™„ë£Œ: {excel_file}")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {(datetime.now().timestamp() - start_time) / 60:.1f}ë¶„")
    
    clear_checkpoints()
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()