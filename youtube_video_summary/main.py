"""
YouTube ì±„ë„ ë™ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ë° ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸
Whisper(ë¡œì»¬ STT) + Gemini(í…ìŠ¤íŠ¸ ìš”ì•½) ë°©ì‹
"""

import json
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import time
from datetime import datetime, timedelta
from typing import List, Dict

import yt_dlp
from google import genai
from google.genai.types import HttpOptions

import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
from openpyxl.utils import get_column_letter

from faster_whisper import WhisperModel

# YouTube ì±„ë„ ëª©ë¡
CHANNELS = [
    "https://www.youtube.com/@yakstory119",
    "https://www.youtube.com/@Ojingeryaksa",
    "https://www.youtube.com/@ë¦¬í‹€ì•½ì‚¬",
    "https://www.youtube.com/@ì•½ì‚¬ë©”ë””ìŠ¨ë§¨",
    "https://www.youtube.com/@ì–‘ê³¼ì",
    "https://www.youtube.com/@HongSee_yaksa"
]

# ì¤‘ê°„ ì €ì¥ íŒŒì¼ ê²½ë¡œ
CHECKPOINT_DIR = "checkpoints"
VIDEOS_CHECKPOINT = os.path.join(CHECKPOINT_DIR, "collected_videos.json")
SUMMARIES_CHECKPOINT = os.path.join(CHECKPOINT_DIR, "summaries.json")
TRANSCRIPTS_CHECKPOINT = os.path.join(CHECKPOINT_DIR, "transcripts.csv")

# ì˜¤ë””ì˜¤ ì €ì¥ ë””ë ‰í† ë¦¬
AUDIO_DIR = "audio"

# Whisper ëª¨ë¸ ìºì‹œ
_WHISPER_MODEL = None


def ensure_checkpoint_dir():
    if not os.path.exists(CHECKPOINT_DIR):
        os.makedirs(CHECKPOINT_DIR)


def ensure_audio_dir():
    if not os.path.exists(AUDIO_DIR):
        os.makedirs(AUDIO_DIR)


def save_videos_checkpoint(videos: List[Dict]):
    ensure_checkpoint_dir()
    with open(VIDEOS_CHECKPOINT, 'w', encoding='utf-8') as f:
        json.dump(videos, f, ensure_ascii=False, indent=2)
    print(f"   ğŸ’¾ ë™ì˜ìƒ ì •ë³´ ì €ì¥: {len(videos)}ê°œ")


def load_videos_checkpoint() -> List[Dict]:
    if os.path.exists(VIDEOS_CHECKPOINT):
        with open(VIDEOS_CHECKPOINT, 'r', encoding='utf-8') as f:
            videos = json.load(f)
        print(f"   ğŸ“‚ ê¸°ì¡´ ë™ì˜ìƒ ë¡œë“œ: {len(videos)}ê°œ")
        return videos
    return []


def save_summaries_checkpoint(summaries: List[Dict]):
    ensure_checkpoint_dir()
    with open(SUMMARIES_CHECKPOINT, 'w', encoding='utf-8') as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)
    print(f"   ğŸ’¾ ìš”ì•½ ê²°ê³¼ ì €ì¥: {len(summaries)}ê°œ")


def load_summaries_checkpoint() -> List[Dict]:
    if os.path.exists(SUMMARIES_CHECKPOINT):
        with open(SUMMARIES_CHECKPOINT, 'r', encoding='utf-8') as f:
            summaries = json.load(f)
        print(f"   ğŸ“‚ ê¸°ì¡´ ìš”ì•½ ë¡œë“œ: {len(summaries)}ê°œ")
        return summaries
    return []


def save_transcripts_checkpoint(transcripts: Dict[str, str]):
    """transcriptë¥¼ CSVë¡œ ì €ì¥"""
    ensure_checkpoint_dir()
    data = [{'video_id': vid, 'transcript': text, 'length': len(text)} 
            for vid, text in transcripts.items()]
    df = pd.DataFrame(data)
    df.to_csv(TRANSCRIPTS_CHECKPOINT, index=False, encoding='utf-8-sig')
    print(f"   ğŸ’¾ transcript ì €ì¥: {len(transcripts)}ê°œ")


def load_transcripts_checkpoint() -> Dict[str, str]:
    """ì €ì¥ëœ transcript ë¶ˆëŸ¬ì˜¤ê¸°"""
    if os.path.exists(TRANSCRIPTS_CHECKPOINT):
        df = pd.read_csv(TRANSCRIPTS_CHECKPOINT, encoding='utf-8-sig')
        transcripts = dict(zip(df['video_id'], df['transcript']))
        print(f"   ğŸ“‚ ê¸°ì¡´ transcript ë¡œë“œ: {len(transcripts)}ê°œ")
        return transcripts
    return {}


def clear_checkpoints():
    files_to_remove = [VIDEOS_CHECKPOINT, SUMMARIES_CHECKPOINT, TRANSCRIPTS_CHECKPOINT]
    for filepath in files_to_remove:
        if os.path.exists(filepath):
            os.remove(filepath)
    print(f"   ğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì™„ë£Œ")


def get_channel_videos(channel_url: str, days_back: int = 30) -> List[Dict]:
    """ì±„ë„ì˜ ìµœê·¼ ë™ì˜ìƒ ì •ë³´ ìˆ˜ì§‘"""
    cutoff_date = datetime.now() - timedelta(days=days_back)
    
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': False,
        'playlistend': 50,
        'skip_download': True,
        'ignoreerrors': True,
        'noprogress': True,
    }
    
    videos = []
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            print(f"   â³ ì±„ë„ ì •ë³´ ë¡œë”© ì¤‘...")
            result = ydl.extract_info(f"{channel_url}/videos", download=False)
            
            if 'entries' in result:
                channel_name = result.get('channel', 'Unknown')
                print(f"   âœ… ì±„ë„: {channel_name}")
                
                entries = [e for e in result['entries'] if e]
                collected = 0
                
                for entry in entries:
                    try:
                        if entry.get('availability') == 'subscriber_only':
                            continue
                        
                        upload_date = datetime.strptime(entry.get('upload_date', '20000101'), '%Y%m%d')
                        
                        if upload_date >= cutoff_date:
                            video_id = entry.get('id', '')
                            videos.append({
                                'channel': channel_name,
                                'channel_url': channel_url,
                                'title': entry.get('title', ''),
                                'video_id': video_id,
                                'url': f"https://www.youtube.com/watch?v={video_id}",
                                'upload_date': entry.get('upload_date', ''),
                                'view_count': entry.get('view_count', 0),
                                'duration': entry.get('duration', 0),
                                'description': entry.get('description', '')[:500],
                            })
                            collected += 1
                    except:
                        continue
                
                print(f"   âœ… ìˆ˜ì§‘: {collected}ê°œ")
    
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {str(e)[:100]}")
    
    return videos


def download_audio_for_video(video_info: Dict) -> str:
    """ì˜¤ë””ì˜¤ë§Œ ë‹¤ìš´ë¡œë“œ"""
    ensure_audio_dir()
    video_url = video_info["url"]
    video_id = video_info["video_id"]
    
    ydl_opts = {
        "quiet": True,
        "no_warnings": True,
        "format": "bestaudio/best",
        "outtmpl": os.path.join(AUDIO_DIR, f"{video_id}.%(ext)s"),
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        print(f"   ğŸ§ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        info = ydl.extract_info(video_url, download=True)
        filename = ydl.prepare_filename(info)
    
    return filename


def get_whisper_model(model_size: str = "small") -> WhisperModel:
    """Whisper ëª¨ë¸ ë¡œë“œ (1íšŒë§Œ)"""
    global _WHISPER_MODEL
    if _WHISPER_MODEL is None:
        print(f"   ğŸ§  Whisper ëª¨ë¸ ë¡œë”©... (size={model_size})")
        _WHISPER_MODEL = WhisperModel(model_size, device="cpu", compute_type="int8")
        print(f"   âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return _WHISPER_MODEL


def transcribe_with_whisper(audio_path: str, model_size: str = "small") -> str:
    """Whisper STT"""
    print(f"   ğŸ“ STT ì§„í–‰ ì¤‘...")
    model = get_whisper_model(model_size=model_size)

    segments, info = model.transcribe(
        audio_path,
        language="ko",
        beam_size=5
    )

    # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ segmentsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    print(f"   â³ ìŒì„± ê¸¸ì´: ì•½ {info.duration:.0f}ì´ˆ")
    
    texts = []
    segment_count = 0
    last_progress = 0
    
    for seg in segments:
        texts.append(seg.text.strip())
        segment_count += 1
        
        # ëŒ€ëµì ì¸ ì§„í–‰ë¥  í‘œì‹œ (10% ë‹¨ìœ„)
        if info.duration > 0:
            current_progress = int((seg.end / info.duration) * 100)
            if current_progress >= last_progress + 10:
                print(f"      ì§„í–‰: {current_progress}% ({seg.end:.0f}/{info.duration:.0f}ì´ˆ)")
                last_progress = current_progress
    
    transcript = " ".join(texts)
    print(f"   âœ… STT ì™„ë£Œ ({len(transcript)}ì, {segment_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ (ìš©ëŸ‰ ì ˆì•½)
    try:
        os.remove(audio_path)
        print(f"   ğŸ—‘ï¸  ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ")
    except Exception as e:
        print(f"   âš ï¸  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
    return transcript


def summarize_video_with_gemini(video_info: Dict, transcript: str) -> str:
    """Geminië¡œ í…ìŠ¤íŠ¸ ìš”ì•½"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        return "ìš”ì•½ ì‹¤íŒ¨: GEMINI_API_KEY ë¯¸ì„¤ì •"
    
    client = genai.Client(
        api_key=api_key,
        http_options=HttpOptions(api_version="v1")
    )
    
    prompt = f"""ì•½ì‚¬ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ - ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì •ë³´ ì¶”ì¶œ

ì œëª©: {video_info['title']}
ì±„ë„: {video_info['channel']}

[ë°›ì•„ì“°ê¸° ë‚´ìš©]
{transcript}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½:

**1. í•µì‹¬ ì£¼ì œ** (1ë¬¸ì¥)

**2. ì œí’ˆ/ì„±ë¶„ ì •ë³´**
ğŸ“¦ ì œí’ˆëª…: [ë¸Œëœë“œ+ì œí’ˆëª…] ë˜ëŠ” "ì–¸ê¸‰ ì—†ìŒ"
ğŸ§ª ì„±ë¶„ëª…: [êµ¬ì²´ì  ì„±ë¶„] ë˜ëŠ” "ì–¸ê¸‰ ì—†ìŒ"  
ğŸ’Š ìš©ë„: [ê±´ê°• ëª©ì ]
ğŸ‘¨â€âš•ï¸ ì•½ì‚¬ ì˜ê²¬: [ì¶”ì²œ/ì£¼ì˜/ì¤‘ë¦½]

**3. ì£¼ìš” ë‚´ìš©** (3-5ê°œ í¬ì¸íŠ¸)

**4. íŠ¸ë Œë“œ ì‹œì‚¬ì ** (1-2ë¬¸ì¥)
"""
    
    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        return response.text
    except Exception as e:
        if "429" in str(e) or "quota" in str(e).lower():
            return "ìš”ì•½ ì‹¤íŒ¨: ì¿¼í„° ì´ˆê³¼"
        return f"ìš”ì•½ ì‹¤íŒ¨: {str(e)[:200]}"


def save_to_excel(all_videos: List[Dict], summaries: List[Dict], output_filename: str):
    """ì—‘ì…€ ì €ì¥"""
    print(f"\nğŸ“Š ì—‘ì…€ ìƒì„± ì¤‘...")
    
    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
        # 1. ì „ì²´ ë™ì˜ìƒ
        df_all = pd.DataFrame(all_videos)
        df_all['ì—…ë¡œë“œì¼'] = pd.to_datetime(df_all['upload_date'], format='%Y%m%d').dt.strftime('%Y-%m-%d')
        
        df_display = df_all[['channel', 'title', 'ì—…ë¡œë“œì¼', 'view_count', 'duration', 'url']].copy()
        df_display.columns = ['ì±„ë„', 'ì œëª©', 'ì—…ë¡œë“œì¼', 'ì¡°íšŒìˆ˜', 'ê¸¸ì´(ì´ˆ)', 'URL']
        df_display['ì¡°íšŒìˆ˜'] = df_display['ì¡°íšŒìˆ˜'].apply(lambda x: f"{x:,}")
        df_display.to_excel(writer, sheet_name='ì „ì²´ ë™ì˜ìƒ ëª©ë¡', index=False)
        
        # 2. AI ìš”ì•½
        if summaries:
            summary_data = []
            for item in summaries:
                v = item['video_info']
                summary_data.append({
                    'ì±„ë„': v['channel'],
                    'ì œëª©': v['title'],
                    'ì—…ë¡œë“œì¼': datetime.strptime(v['upload_date'], '%Y%m%d').strftime('%Y-%m-%d'),
                    'ì¡°íšŒìˆ˜': f"{v['view_count']:,}",
                    'URL': v['url'],
                    'AI ìš”ì•½': item['summary']
                })
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='AI ìš”ì•½ ê²°ê³¼', index=False)
        
        # 3. ì±„ë„ í†µê³„
        stats = df_all.groupby('channel').agg({
            'title': 'count',
            'view_count': ['sum', 'mean', 'max'],
            'duration': 'sum'
        }).reset_index()
        stats.columns = ['ì±„ë„', 'ë™ì˜ìƒ ìˆ˜', 'ì´ ì¡°íšŒìˆ˜', 'í‰ê·  ì¡°íšŒìˆ˜', 'ìµœëŒ€ ì¡°íšŒìˆ˜', 'ì´ ê¸¸ì´(ì´ˆ)']
        for col in ['ì´ ì¡°íšŒìˆ˜', 'í‰ê·  ì¡°íšŒìˆ˜', 'ìµœëŒ€ ì¡°íšŒìˆ˜']:
            stats[col] = stats[col].apply(lambda x: f"{int(x):,}")
        stats['ì´ ê¸¸ì´(ë¶„)'] = (stats['ì´ ê¸¸ì´(ì´ˆ)'].astype(int) / 60).round(1)
        stats = stats.drop('ì´ ê¸¸ì´(ì´ˆ)', axis=1)
        stats.to_excel(writer, sheet_name='ì±„ë„ë³„ í†µê³„', index=False)
    
    print(f"   âœ… ì—‘ì…€ ìƒì„± ì™„ë£Œ")


def main():
    start_time = datetime.now().timestamp()
    
    print("=" * 80)
    print("ğŸ¬ YouTube ì•½ì‚¬ ì±„ë„ ë¶„ì„ ì‹œìŠ¤í…œ (Whisper + Gemini)")
    print("=" * 80)
    
    days_back = 30
    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days_back}ì¼")
    print(f"ğŸ“º ëŒ€ìƒ ì±„ë„: {len(CHANNELS)}ê°œ")
    
    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    existing_videos = load_videos_checkpoint()
    existing_summaries = load_summaries_checkpoint()
    existing_transcripts = load_transcripts_checkpoint()
    
    if existing_videos or existing_summaries:
        print(f"\nâš ï¸  ì´ì „ ì‘ì—… ë°œê²¬")
        print(f"   ë™ì˜ìƒ: {len(existing_videos)}ê°œ")
        print(f"   ìš”ì•½: {len(existing_summaries)}ê°œ")
        print(f"   transcript: {len(existing_transcripts)}ê°œ")
        response = input("ì´ì–´ì„œ ì§„í–‰? (y/n): ").lower()
        if response == 'y':
            all_videos = existing_videos
            summaries = existing_summaries
            transcripts = existing_transcripts
            skip_collection = True
        else:
            clear_checkpoints()
            all_videos = []
            summaries = []
            transcripts = {}
            skip_collection = False
    else:
        all_videos = []
        summaries = []
        transcripts = {}
        skip_collection = False
    
    # 1ë‹¨ê³„: ë™ì˜ìƒ ìˆ˜ì§‘
    if not skip_collection:
        print("\n" + "=" * 80)
        print("ğŸ” 1ë‹¨ê³„: ë™ì˜ìƒ ì •ë³´ ìˆ˜ì§‘")
        print("=" * 80)
        
        for idx, channel_url in enumerate(CHANNELS, 1):
            print(f"\n[{idx}/{len(CHANNELS)}] {channel_url.split('@')[1]}")
            videos = get_channel_videos(channel_url, days_back)
            all_videos.extend(videos)
            save_videos_checkpoint(all_videos)
        
        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(all_videos)}ê°œ")
        all_videos.sort(key=lambda x: x['upload_date'], reverse=True)
    
    # 2ë‹¨ê³„: STT + ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ¤– 2ë‹¨ê³„: STT + AI ìš”ì•½")
    print("=" * 80)
    
    summarized_ids = {s['video_info']['video_id'] for s in summaries}
    videos_to_summarize = [v for v in all_videos if v['video_id'] not in summarized_ids]
    
    print(f"\nì™„ë£Œ: {len(summaries)}, ëŒ€ê¸°: {len(videos_to_summarize)}")
    
    if videos_to_summarize:
        print(f"â±ï¸  ì˜ˆìƒ ì‹œê°„: ì˜ìƒë‹¹ 2-5ë¶„")
        print(f"ğŸ”§ Whisper(ë¡œì»¬) + Gemini(í…ìŠ¤íŠ¸ ìš”ì•½)")
        
        for i, video in enumerate(videos_to_summarize, 1):
            video_id = video['video_id']
            current_index = len(summaries) + i
            print(f"\n[{current_index}/{len(all_videos)}] {video['title'][:50]}")
            
            try:
                # STT: ê¸°ì¡´ transcript ìˆìœ¼ë©´ ì¬ì‚¬ìš©
                if video_id in transcripts:
                    print(f"   â™»ï¸  ê¸°ì¡´ transcript ì¬ì‚¬ìš©")
                    transcript = transcripts[video_id]
                else:
                    audio_path = download_audio_for_video(video)
                    transcript = transcribe_with_whisper(audio_path)
                    transcripts[video_id] = transcript
                    save_transcripts_checkpoint(transcripts)
                
                # ìš”ì•½
                summary = summarize_video_with_gemini(video, transcript)
            except Exception as e:
                summary = f"ìš”ì•½ ì‹¤íŒ¨: {e}"
            
            summaries.append({
                'video_info': video,
                'summary': summary,
                'processed_at': datetime.now().isoformat()
            })
            
            save_summaries_checkpoint(summaries)
    
    # ê²°ê³¼ ì €ì¥
    excel_file = f'youtube_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx'
    save_to_excel(all_videos, summaries, excel_file)
    
    print(f"\nâœ… ì™„ë£Œ: {excel_file}")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {(datetime.now().timestamp() - start_time) / 60:.1f}ë¶„")
    
    clear_checkpoints()
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()