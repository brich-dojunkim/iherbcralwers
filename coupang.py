import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import time
import json
import csv
import random
import re
from datetime import datetime
import os
import platform
import requests
from PIL import Image

class CoupangCrawlerMacOS:
    def __init__(self, headless=False, delay_range=(2, 5)):
        """
        macOS ìµœì í™” ì¿ íŒ¡ í¬ë¡¤ëŸ¬ - ì´ë¯¸ì§€ ìˆ˜ì§‘ ê¸°ëŠ¥ ì¶”ê°€
        """
        self.headless = headless
        self.delay_range = delay_range
        self.driver = None
        self.products = []
        
        # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.images_dir = "coupang_images"
        os.makedirs(self.images_dir, exist_ok=True)
        self.downloaded_images = {}  # product_id -> image_path
        
        # Chrome ì˜µì…˜ ì„¤ì • (macOS ìµœì í™”)
        self.options = uc.ChromeOptions()
        
        # macOSì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ì˜µì…˜ë“¤ë§Œ ì‚¬ìš©
        self.options.add_argument('--no-sandbox')
        self.options.add_argument('--disable-dev-shm-usage')
        self.options.add_argument('--disable-blink-features=AutomationControlled')
        self.options.add_argument('--disable-extensions')
        self.options.add_argument('--disable-plugins-discovery')
        
        # ì‹¤í—˜ì  ì˜µì…˜ (macOS í˜¸í™˜)
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agent ì„¤ì •
        self.options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        if headless:
            self.options.add_argument('--headless')
            
        # ìœˆë„ìš° í¬ê¸° ì„¤ì •
        self.options.add_argument('--window-size=1920,1080')
    
    def start_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì‹œì‘ (macOS ìµœì í™”)"""
        try:
            print("Chrome ë“œë¼ì´ë²„ ì‹œì‘ ì¤‘... (macOS)")
            
            # macOSì—ì„œëŠ” ë²„ì „ ê°ì§€ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ë„ ìˆìŒ
            self.driver = uc.Chrome(
                options=self.options,
                version_main=None,  # ìë™ ê°ì§€
                driver_executable_path=None,  # ìë™ ë‹¤ìš´ë¡œë“œ
                browser_executable_path=None,  # ì‹œìŠ¤í…œ Chrome ì‚¬ìš©
            )
            
            # JavaScriptë¡œ ì›¹ë“œë¼ì´ë²„ ì†ì„± ìˆ¨ê¸°ê¸°
            self.driver.execute_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['ko-KR', 'ko', 'en-US', 'en']
                });
                
                window.chrome = {
                    runtime: {}
                };
            """)
            
            print("Chrome ë“œë¼ì´ë²„ ì‹œì‘ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"ë“œë¼ì´ë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            
            # ëŒ€ì²´ ë°©ë²• ì‹œë„
            try:
                print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ë“œë¼ì´ë²„ ì‹œì‘ ì‹œë„...")
                # ë” ë‹¨ìˆœí•œ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„
                simple_options = uc.ChromeOptions()
                simple_options.add_argument('--no-sandbox')
                simple_options.add_argument('--disable-dev-shm-usage')
                
                if self.headless:
                    simple_options.add_argument('--headless')
                
                self.driver = uc.Chrome(options=simple_options)
                print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ë“œë¼ì´ë²„ ì‹œì‘ ì„±ê³µ")
                return True
                
            except Exception as e2:
                print(f"ëŒ€ì²´ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                return False
    
    def human_like_scroll(self):
        """ì¸ê°„ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤ë§"""
        try:
            # í˜ì´ì§€ ë†’ì´ ê°€ì ¸ì˜¤ê¸°
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            viewport_height = self.driver.execute_script("return window.innerHeight")
            
            current_position = 0
            scroll_step = random.randint(200, 400)
            
            # í˜ì´ì§€ë¥¼ ì²œì²œíˆ ìŠ¤í¬ë¡¤ ë‹¤ìš´
            while current_position < total_height - viewport_height:
                scroll_distance = random.randint(scroll_step - 50, scroll_step + 50)
                current_position += scroll_distance
                
                self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                time.sleep(random.uniform(0.3, 0.8))
            
            # ì ì‹œ ëŒ€ê¸° í›„ ìƒë‹¨ìœ¼ë¡œ
            time.sleep(random.uniform(1, 2))
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(random.uniform(0.5, 1))
            
        except Exception as e:
            print(f"ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def wait_for_page_load(self, timeout=20):
        """í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì¦ê°€)"""
        try:
            # ì—¬ëŸ¬ ì¡°ê±´ì„ ì²´í¬í•´ì„œ í˜ì´ì§€ ë¡œë”© í™•ì¸
            wait = WebDriverWait(self.driver, timeout)
            
            # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ ëŒ€ê¸°
            wait.until(lambda driver: 
                driver.find_elements(By.ID, "product-list") or 
                driver.find_elements(By.CLASS_NAME, "search-no-result") or
                "ì°¨ë‹¨" in driver.title.lower()
            )
            
            # DOM ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
            self.driver.execute_script("return document.readyState === 'complete'")
            
            # ì¶”ê°€ ë¡œë”© ì‹œê°„
            time.sleep(random.uniform(2, 4))
            
            # ì°¨ë‹¨ í˜ì´ì§€ í™•ì¸
            if "ì°¨ë‹¨" in self.driver.title.lower() or "blocked" in self.driver.title.lower():
                print("âš ï¸ ì°¨ë‹¨ í˜ì´ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ ëŒ€ê¸°í•´ì£¼ì„¸ìš”.")
                time.sleep(10)
                return False
            
            return True
            
        except TimeoutException:
            print("í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
            return False
    
    def extract_image_url_from_element(self, product_item):
        """ìƒí’ˆ í•­ëª©ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        try:
            # ì—¬ëŸ¬ ì´ë¯¸ì§€ ì„ íƒì ì‹œë„
            image_selectors = [
                "figure.ProductUnit_productImage__Mqcg1 img",
                ".ProductUnit_productImage img",
                "img[src*='thumbnail']",
                "img[src*='coupangcdn.com']",
                "img"
            ]
            
            for selector in image_selectors:
                try:
                    img_element = product_item.find(selector)
                    if img_element:
                        img_url = img_element.get('src')
                        if img_url and img_url.startswith('http') and 'coupangcdn.com' in img_url:
                            return img_url
                except:
                    continue
            
            return None
            
        except Exception as e:
            print(f"ì´ë¯¸ì§€ URL ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def download_product_image(self, product_id, image_url):
        """ìƒí’ˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            if not image_url:
                return None
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': 'https://www.coupang.com/'
            }
            
            response = requests.get(image_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # íŒŒì¼ëª… ìƒì„±
            filename = f"coupang_{product_id}.jpg"
            filepath = os.path.join(self.images_dir, filename)
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            # ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬
            try:
                with Image.open(filepath) as img:
                    img.verify()
                return filepath
            except:
                # ì†ìƒëœ íŒŒì¼ ì‚­ì œ
                if os.path.exists(filepath):
                    os.remove(filepath)
                return None
                
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({product_id}): {e}")
            return None
    
    def extract_product_info(self, product_item):
        """ê°œë³„ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            product = {}
            
            # ìƒí’ˆ ID
            product_id = product_item.get('data-id', '')
            product['product_id'] = product_id
            
            # ìƒí’ˆ ë§í¬
            link_element = product_item.find('a')
            if link_element and link_element.get('href'):
                product['product_url'] = 'https://www.coupang.com' + link_element.get('href')
            else:
                product['product_url'] = ''
            
            # ìƒí’ˆëª…
            name_element = product_item.find('div', class_='ProductUnit_productName__gre7e')
            product['product_name'] = name_element.get_text(strip=True) if name_element else ''
            
            # ê°€ê²© ì •ë³´
            price_area = product_item.find('div', class_='PriceArea_priceArea__NntJz')
            if price_area:
                # í˜„ì¬ ê°€ê²©
                current_price_elem = price_area.find('strong', class_='Price_priceValue__A4KOr')
                product['current_price'] = current_price_elem.get_text(strip=True) if current_price_elem else ''
                
                # ì›ë˜ ê°€ê²©
                original_price_elem = price_area.find('del', class_='PriceInfo_basePrice__8BQ32')
                product['original_price'] = original_price_elem.get_text(strip=True) if original_price_elem else ''
                
                # í• ì¸ìœ¨
                discount_elem = price_area.find('span', class_='PriceInfo_discountRate__EsQ8I')
                product['discount_rate'] = discount_elem.get_text(strip=True) if discount_elem else ''
            
            # í‰ì  ë° ë¦¬ë·°
            rating_area = product_item.find('div', class_='ProductRating_productRating__jjf7W')
            if rating_area:
                # í‰ì 
                rating_elem = rating_area.find('div', class_='ProductRating_star__RGSlV')
                if rating_elem:
                    width_style = rating_elem.get('style', '')
                    width_match = re.search(r'width:\s*(\d+)%', width_style)
                    if width_match:
                        rating_percent = int(width_match.group(1))
                        product['rating'] = round(rating_percent / 20, 1)
                
                # ë¦¬ë·° ìˆ˜
                review_count_elem = rating_area.find('span', class_='ProductRating_ratingCount__R0Vhz')
                if review_count_elem:
                    review_text = review_count_elem.get_text(strip=True)
                    review_number = re.sub(r'[^\d]', '', review_text)
                    product['review_count'] = review_number
            
            # ë°°ì†¡ ì •ë³´
            delivery_badge = product_item.find('div', class_='TextBadge_delivery__STgTC')
            product['delivery_badge'] = delivery_badge.get_text(strip=True) if delivery_badge else ''
            
            # ë¡œì¼“ì§êµ¬ ì—¬ë¶€
            rocket_img = product_item.find('img', alt='ë¡œì¼“ì§êµ¬')
            product['is_rocket'] = rocket_img is not None
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ ë° ë‹¤ìš´ë¡œë“œ
            image_url = self.extract_image_url_from_element(product_item)
            product['image_url'] = image_url if image_url else ''
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„
            if image_url and product_id:
                image_path = self.download_product_image(product_id, image_url)
                if image_path:
                    product['image_path'] = image_path
                    self.downloaded_images[product_id] = image_path
                    print(f"    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {product_id}")
                else:
                    product['image_path'] = ''
                    print(f"    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {product_id}")
            else:
                product['image_path'] = ''
            
            # í¬ë¡¤ë§ ì‹œê°„
            product['crawled_at'] = datetime.now().isoformat()
            
            return product
            
        except Exception as e:
            print(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}
    
    def extract_products_from_current_page(self):
        """í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ìƒí’ˆ ì¶”ì¶œ + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ì°¨ë‹¨ í™•ì¸
            if "ì°¨ë‹¨" in self.driver.title.lower():
                print("âš ï¸ ì°¨ë‹¨ëœ í˜ì´ì§€ì…ë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return []
            
            # ìŠ¤í¬ë¡¤ë§
            self.human_like_scroll()
            
            # HTML íŒŒì‹±
            html_content = self.driver.page_source
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
            product_list = soup.find('ul', id='product-list')
            if not product_list:
                print("ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ í™•ì¸
                no_result = soup.find('div', class_='search-no-result')
                if no_result:
                    print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                return []
            
            # ìƒí’ˆ ì¶”ì¶œ
            product_items = product_list.find_all('li', class_='ProductUnit_productUnit__Qd6sv')
            page_products = []
            
            print(f"    í˜ì´ì§€ì—ì„œ {len(product_items)}ê°œ ìƒí’ˆ ë°œê²¬")
            
            for idx, item in enumerate(product_items):
                product = self.extract_product_info(item)
                if product and product.get('product_name'):
                    page_products.append(product)
                    
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê°„ê²© ì¡°ì ˆ
                if idx % 5 == 0:
                    time.sleep(random.uniform(0.5, 1.0))
            
            return page_products
            
        except Exception as e:
            print(f"í˜ì´ì§€ ìƒí’ˆ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def has_next_page(self):
        """ë‹¤ìŒ í˜ì´ì§€ í™•ì¸"""
        try:
            next_button = self.driver.find_element(By.CSS_SELECTOR, 'a.Pagination_nextBtn__TUY5t')
            button_classes = next_button.get_attribute('class')
            return 'disabled' not in button_classes
        except:
            return False
    
    def go_to_next_page(self):
        """ë‹¤ìŒ í˜ì´ì§€ ì´ë™"""
        try:
            next_button = self.driver.find_element(By.CSS_SELECTOR, 'a.Pagination_nextBtn__TUY5t')
            
            if 'disabled' in next_button.get_attribute('class'):
                return False
            
            # ë²„íŠ¼ ìœ„ì¹˜ë¡œ ìŠ¤í¬ë¡¤
            self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", next_button)
            time.sleep(random.uniform(1, 2))
            
            # í´ë¦­
            self.driver.execute_script("arguments[0].click();", next_button)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            time.sleep(random.uniform(3, 6))
            return True
            
        except Exception as e:
            print(f"ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì˜¤ë¥˜: {e}")
            return False
    
    def crawl_all_pages(self, start_url, max_pages=None):
        """ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ + ì´ë¯¸ì§€ ìˆ˜ì§‘"""
        if not self.start_driver():
            return []
        
        try:
            print(f"í¬ë¡¤ë§ ì‹œì‘: {start_url}")
            print("(macOSì—ì„œëŠ” ì•½ê°„ì˜ ì¶”ê°€ ëŒ€ê¸°ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            print(f"ì´ë¯¸ì§€ ì €ì¥ í´ë”: {self.images_dir}")
            
            # ì²« í˜ì´ì§€ ë¡œë“œ
            self.driver.get(start_url)
            
            if not self.wait_for_page_load():
                print("ì²« í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
                return []
            
            page_count = 0
            total_images_downloaded = 0
            
            while True:
                page_count += 1
                print(f"\n=== í˜ì´ì§€ {page_count} í¬ë¡¤ë§ ì¤‘ ===")
                
                # í˜„ì¬ í˜ì´ì§€ ìƒí’ˆ ì¶”ì¶œ (ì´ë¯¸ì§€ í¬í•¨)
                page_products = self.extract_products_from_current_page()
                
                if not page_products and page_count == 1:
                    print("âš ï¸ ì²« í˜ì´ì§€ì—ì„œ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print("í˜„ì¬ í˜ì´ì§€ ì œëª©:", self.driver.title)
                    print("ë¸Œë¼ìš°ì €ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    
                    # 10ì´ˆ ëŒ€ê¸° (ìˆ˜ë™ ì²˜ë¦¬ ì‹œê°„)
                    time.sleep(10)
                    page_products = self.extract_products_from_current_page()
                
                self.products.extend(page_products)
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í†µê³„
                page_images = len([p for p in page_products if p.get('image_path')])
                total_images_downloaded += page_images
                
                print(f"í˜ì´ì§€ {page_count}ì—ì„œ {len(page_products)}ê°œ ìƒí’ˆ ì¶”ì¶œ (ì´ë¯¸ì§€: {page_images}ê°œ)")
                print(f"ì´ ëˆ„ì  ìƒí’ˆ ìˆ˜: {len(self.products)}ê°œ (ì´ë¯¸ì§€: {total_images_downloaded}ê°œ)")
                
                # ìµœëŒ€ í˜ì´ì§€ í™•ì¸
                if max_pages and page_count >= max_pages:
                    print(f"ìµœëŒ€ í˜ì´ì§€ ìˆ˜({max_pages}) ë„ë‹¬")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                if not self.has_next_page():
                    print("ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if not self.go_to_next_page():
                    print("ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                    break
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                if not self.wait_for_page_load():
                    print("í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
                    break
                
                # ëœë¤ ëŒ€ê¸° (macOSì—ì„œëŠ” ì¡°ê¸ˆ ë” ê¸¸ê²Œ)
                wait_time = random.uniform(self.delay_range[0] + 1, self.delay_range[1] + 2)
                print(f"{wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(wait_time)
            
            print(f"\ní¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ì´ {len(self.products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")
            print(f"ì´ {total_images_downloaded}ê°œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ")
            
            return self.products
            
        except KeyboardInterrupt:
            print("\nì‚¬ìš©ìê°€ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            return self.products
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self.products
        
        finally:
            try:
                if self.driver:
                    print("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘...")
                    self.driver.quit()
            except:
                pass
    
    def save_to_csv(self, filename=None):
        """CSV ì €ì¥ - ì´ë¯¸ì§€ ì •ë³´ í¬í•¨"""
        if not self.products:
            print("ì €ì¥í•  ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'coupang_products_{timestamp}.csv'
        
        fieldnames = [
            'product_id', 'product_name', 'product_url',
            'current_price', 'original_price', 'discount_rate',
            'rating', 'review_count', 'delivery_badge',
            'is_rocket', 'image_url', 'image_path', 'crawled_at'
        ]
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for product in self.products:
                    row = {field: product.get(field, '') for field in fieldnames}
                    writer.writerow(row)
            
            print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            
            # ì´ë¯¸ì§€ ë§¤í•‘ JSON ì €ì¥
            if self.downloaded_images:
                mapping_filename = f'coupang_image_mapping_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                with open(mapping_filename, 'w', encoding='utf-8') as f:
                    json.dump(self.downloaded_images, f, ensure_ascii=False, indent=2)
                print(f"âœ… ì´ë¯¸ì§€ ë§¤í•‘ ì €ì¥ ì™„ë£Œ: {mapping_filename}")
            
            return filename
            
        except Exception as e:
            print(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ - ì´ë¯¸ì§€ ì •ë³´ í¬í•¨"""
        if not self.products:
            print("ìˆ˜ì§‘ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\n=== í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ===")
        print(f"ì´ ìƒí’ˆ ìˆ˜: {len(self.products)}ê°œ")
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í†µê³„
        products_with_images = len([p for p in self.products if p.get('image_path')])
        print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {products_with_images}ê°œ ({products_with_images/len(self.products)*100:.1f}%)")
        
        # í‰ì  í†µê³„
        rated_products = [p for p in self.products if p.get('rating') and isinstance(p.get('rating'), (int, float))]
        if rated_products:
            avg_rating = sum(p['rating'] for p in rated_products) / len(rated_products)
            print(f"í‰ê·  í‰ì : {avg_rating:.2f}ì ")
        
        # ë¡œì¼“ì§êµ¬ ìƒí’ˆ
        rocket_count = sum(1 for p in self.products if p.get('is_rocket'))
        print(f"ë¡œì¼“ì§êµ¬ ìƒí’ˆ: {rocket_count}ê°œ")
        
        # ë¬´ë£Œë°°ì†¡ ìƒí’ˆ
        free_shipping = sum(1 for p in self.products if 'ë¬´ë£Œë°°ì†¡' in str(p.get('delivery_badge', '')))
        print(f"ë¬´ë£Œë°°ì†¡ ìƒí’ˆ: {free_shipping}ê°œ")
        
        print(f"ì´ë¯¸ì§€ ì €ì¥ í´ë”: {self.images_dir}")


# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    print("ğŸ macOSìš© ì¿ íŒ¡ í¬ë¡¤ëŸ¬ (ì´ë¯¸ì§€ ìˆ˜ì§‘ í¬í•¨)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = CoupangCrawlerMacOS(
        headless=False,  # macOSì—ì„œëŠ” ì²˜ìŒì—ëŠ” False ê¶Œì¥
        delay_range=(3, 6)  # macOSì—ì„œëŠ” ì¡°ê¸ˆ ë” ë³´ìˆ˜ì ìœ¼ë¡œ
    )
    
    # ê²€ìƒ‰ URL
    search_url = "https://www.coupang.com/np/search?listSize=36&filterType=coupang_global&rating=0&isPriceRange=false&minPrice=&maxPrice=&component=&sorter=scoreDesc&brand=&offerCondition=&filter=194176%23attr_7652%2431823%40DEFAULT&fromComponent=N&channel=user&selectedPlpKeepFilter=&q=%EB%82%98%EC%9A%B0%ED%91%B8%EB%93%9C"
    
    print("ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©´ í•„ìš”ì‹œ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    print("Ctrl+Cë¡œ ì–¸ì œë“ ì§€ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰ (ì´ë¯¸ì§€ ìˆ˜ì§‘ í¬í•¨)
        products = crawler.crawl_all_pages(search_url, max_pages=None)
        
        # ê²°ê³¼ ì €ì¥
        if products:
            csv_filename = crawler.save_to_csv()
            crawler.print_summary()
            
            print(f"\nğŸ‰ ì‘ì—… ì™„ë£Œ!")
            print(f"CSV íŒŒì¼: {csv_filename}")
            print(f"ì´ë¯¸ì§€ í´ë”: {crawler.images_dir}")
            print("ì´ì œ image_experiment_runner.pyë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ í¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¸Œë¼ìš°ì €ì—ì„œ í˜ì´ì§€ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        # ì¤‘ë‹¨ëœ ìƒíƒœì—ì„œë„ ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„° ì €ì¥
        if crawler.products:
            crawler.save_to_csv()
            print("ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ‰ ì‘ì—… ì™„ë£Œ!")