#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì‡¼í•‘ â†’ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ URL ìˆ˜ì§‘ (í‚¤ì›Œë“œ ë°°ì¹˜, ìë™ ì¬ê°œ)
- URL ë‹¨ìœ„ ì¬ê°œ(progress/*.done)
- í‚¤ì›Œë“œë§ˆë‹¤ ë¸Œë¼ìš°ì € ë¦¬ì…‹
- ê¸°ë³¸: í‚¤ì›Œë“œ ì‹œì‘ ì‹œ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™ â†’ ì¸ì¦/ë³´ì•ˆë¬¸ì í•´ê²° â†’ íƒ€ê²Ÿ URL ë³µê·€
- ë¡œê·¸ì¸/ìº¡ì°¨ ê°ì§€ ì‹œ:
  * ê¸°ë³¸(ê¶Œì¥): --auto-resume (ON) â†’ í´ë§ìœ¼ë¡œ ìë™ ì¬ê°œ
  * ì˜µì…˜: --interactive â†’ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸(Enter/force/skip/open)
- ìº¡ì°¨ ë³µê·€ í›„ 'ê»ë°ê¸° DOM' ëŒ€ì‘: ì¬í•˜ì´ë“œë ˆì´ì…˜ ë£¨í‹´(ì œìŠ¤ì²˜/ê°•ìŠ¤í¬ë¡¤/í•„í„°í„°ì¹˜/ë¦¬í”„ë ˆì‹œ) í›„ ì¬ìˆ˜ì§‘
- í˜ì´ì§€ë³„ ë¶€ë¶„ ê²°ê³¼(progress/*.stores.txt) ì €ì¥, ì¢…ë£Œ ì‹œ ë³‘í•©í•˜ì—¬ --outì— ì¤‘ë³µ ì œê±° ì €ì¥

ì˜ˆì‹œ:
  python3 batch_collect.py --url-file urls.txt --pages 10 --out stores_unique.txt
  # ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ
  python3 batch_collect.py --url-file urls.txt --pages 10 --out stores_unique.txt --interactive
"""

import os, re, time, html, json, argparse, sys, hashlib
from pathlib import Path
from typing import List, Set, Iterable, Optional
from urllib.parse import urlparse, parse_qs, unquote, urlencode, urlunparse, quote
from datetime import datetime

import undetected_chromedriver as uc
from selenium.webdriver.common.by   import By
from selenium.webdriver.support.ui  import WebDriverWait
from selenium.webdriver.support     import expected_conditions as EC
from selenium.common.exceptions     import TimeoutException, UnexpectedAlertPresentException, JavascriptException, WebDriverException
from selenium.webdriver import ActionChains
from selenium.webdriver.common.keys import Keys

# ============ ì„¤ì • ============
PAGE_WAIT = 12
SCROLL_MAX_STEPS = 30
SCROLL_PAUSE = 0.35
SCROLL_SETTLE_PAUSE = 0.6
SAFE_GET_MAX_RETRIES = 3
SAFE_GET_STABILIZE_SLEEP = 0.8

USER_DATA_DIR = os.path.expanduser("~/.cache/ossomall_chrome_batch")
os.makedirs(USER_DATA_DIR, exist_ok=True)

PROGRESS_DIR = Path("./progress")
PROGRESS_DIR.mkdir(exist_ok=True)
DEBUG_DIR = Path("./debug")
DEBUG_DIR.mkdir(exist_ok=True)

LOGIN_URL_TMPL = "https://nid.naver.com/nidlogin.login?mode=form&url={return_url}"
GLOBAL_CAPTCHA_HOST_SUBSTRING = "ncpt.naver.com"
GLOBAL_CAPTCHA_INDICATORS = [
    "ìë™ì…ë ¥ ë°©ì§€", "ë³´ì•ˆë¬¸ì", "ë¹„ì •ìƒì ì¸ ì ‘ê·¼", "ì ‘ì†ì´ ì¼ì‹œì ìœ¼ë¡œ ì œí•œ",
    "captcha", "blocked"
]
LOGIN_HOST_SUBSTRING = "nid.naver.com"

SEARCH_SUCCESS_XPATHS = [
    "//div[contains(@class,'basicList_list') or contains(@class,'list_basis') or contains(@class,'product_list')]",
    "//ul[contains(@class,'list_basis') or contains(@class,'list_area')]",
    "//a[contains(@href,'smartstore.naver.com')]"
]

SMARTSTORE_BASE_RE = re.compile(r"https?://smartstore\.naver\.com/([A-Za-z0-9._-]+)", re.I)
OUTLINK_RE = re.compile(
    r"https?://smartstore\.naver\.com/inflow/outlink/url\?[^\"'>]*\burl=([^\"'&]+)",
    re.I
)

# ============ ìœ í‹¸ ============
def normalize_url(u: Optional[str]) -> Optional[str]:
    if not u: return None
    u = u.strip()
    if not u: return None
    if not u.startswith(("http://", "https://")):
        u = "https://" + u.lstrip("/")
    return u

def with_paging_index(url: str, index: int) -> str:
    pr = urlparse(url)
    qs = parse_qs(pr.query, keep_blank_values=True)
    qs["pagingIndex"] = [str(index)]
    new_q = urlencode(qs, doseq=True)
    return urlunparse((pr.scheme, pr.netloc, pr.path, pr.params, new_q, pr.fragment))

def to_store_base(url: str) -> Optional[str]:
    m = SMARTSTORE_BASE_RE.search(url.strip())
    if not m: return None
    return f"https://smartstore.naver.com/{m.group(1)}"

def extract_from_outlink(url: str) -> Optional[str]:
    m = OUTLINK_RE.search(url)
    if not m:
        return None
    raw = m.group(1)
    decoded = html.unescape(unquote(raw))
    return to_store_base(decoded)

def url_hash(url: str) -> str:
    return hashlib.md5(url.encode("utf-8")).hexdigest()[:12]

def paths_for_url(idx: int, url: str):
    h = url_hash(url)
    base = f"{idx:04d}_{h}"
    done = PROGRESS_DIR / f"{base}.done"
    stores_txt = PROGRESS_DIR / f"{base}.stores.txt"
    return done, stores_txt, base

# ============ DOM/ìƒíƒœ íŒì • ============
def page_contains_any(driver, xpaths: List[str]) -> bool:
    try:
        for xp in xpaths:
            if driver.find_elements(By.XPATH, xp):
                return True
    except Exception:
        pass
    return False

def search_dom_looks_ok(driver) -> bool:
    return page_contains_any(driver, SEARCH_SUCCESS_XPATHS)

def is_login_redirect(driver) -> bool:
    try:
        return LOGIN_HOST_SUBSTRING in (driver.current_url or "").lower()
    except Exception:
        return False

def is_global_captcha_page(driver) -> bool:
    try:
        cur = (driver.current_url or "").lower()
        htmlsrc = (driver.page_source or "").lower()
        looks_like_captcha = (GLOBAL_CAPTCHA_HOST_SUBSTRING in cur) or any(ind in htmlsrc for ind in GLOBAL_CAPTCHA_INDICATORS)
        if looks_like_captcha and search_dom_looks_ok(driver):
            return False
        return looks_like_captcha
    except Exception:
        return False

def _dump_debug_snapshot(driver, prefix="captcha"):
    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        cur = (driver.current_url or "about:blank")
        title = (driver.title or "").strip()
        html_src = driver.page_source or ""
        p = DEBUG_DIR / f"{prefix}_{ts}.html"
        p.write_text(html_src, encoding="utf-8")
        print(f"[DEBUG] current_url: {cur}")
        print(f"[DEBUG] title      : {title}")
        print(f"[DEBUG] snapshot   : {p}")
    except Exception as e:
        print(f"[DEBUG] snapshot error: {e}")

# ============ ëŒ€ê¸°/ì¬ê°œ: ìë™ í´ë§ & ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ============
def auto_wait_issue_clear(driver, expected_url: Optional[str], reason: str,
                          poll_interval: float, poll_timeout: int,
                          auto_refresh_every: int = 30) -> bool:
    """
    ìë™ í´ë§ìœ¼ë¡œ ë¡œê·¸ì¸/ìº¡ì°¨ í•´ì œ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³ , í•´ì œë˜ë©´ True.
    poll_timeout(ì´ˆ) ë‚´ í•´ì œ ì•ˆ ë˜ë©´ False.
    """
    print(f"\nğŸ”’ {reason} ìƒíƒœ ê°ì§€ â†’ ìë™ í´ë§ìœ¼ë¡œ í•´ì œ ê°ì‹œ ì‹œì‘ "
          f"(interval={poll_interval}s, timeout={poll_timeout}s)")
    _dump_debug_snapshot(driver, prefix=reason.replace('/', '_'))

    start = time.time()
    last_refresh = time.time()
    while True:
        # íƒ€ì„ì•„ì›ƒ
        if time.time() - start > poll_timeout:
            print("â±ï¸ íƒ€ì„ì•„ì›ƒ: ìë™ ì¬ê°œ ì‹¤íŒ¨")
            return False

        # ì£¼ê¸°ì  ìƒˆë¡œê³ ì¹¨ ì‹œë„(ì˜µì…˜)
        if expected_url and (time.time() - last_refresh) >= auto_refresh_every:
            try:
                driver.get(expected_url)
                last_refresh = time.time()
            except Exception:
                pass

        time.sleep(poll_interval)

        # í•´ì œ í™•ì¸
        if not is_login_redirect(driver) and not is_global_captcha_page(driver):
            # ê²€ìƒ‰ê²°ê³¼ë©´ DOMë„ í™•ì¸
            if "search.shopping.naver.com" in (driver.current_url or ""):
                if search_dom_looks_ok(driver):
                    print("âœ… ìë™ ê°ì§€: ì •ìƒ DOM í™•ì¸ â†’ ìˆ˜ì§‘ ì¬ê°œ")
                    return True
                else:
                    # ì•„ì§ í•˜ì´ë“œë ˆì´ì…˜ ì „ì¼ ìˆ˜ ìˆìŒ â†’ ë‹¤ìŒ ë£¨í”„
                    continue
            else:
                print("âœ… ìë™ ê°ì§€: ë¬¸ì œ í•´ì œ â†’ ìˆ˜ì§‘ ì¬ê°œ")
                return True

def prompt_wait_issue_clear(driver, expected_url: Optional[str], reason: str):
    """
    ê¸°ì¡´ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ.
    """
    while True:
        print(f"\nğŸ”’ {reason} ìƒíƒœ ê°ì§€")
        _dump_debug_snapshot(driver, prefix=reason.replace('/', '_'))
        print("ë¸Œë¼ìš°ì €ì—ì„œ ë¬¸ì œ(ìº¡ì°¨/ë¡œê·¸ì¸)ë¥¼ í•´ê²°í•œ ë’¤, ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("  [Enter] ë‹¤ì‹œ ê²€ì‚¬ / [force] ê°•ì œ ì§„í–‰ / [skip] ì´ í‚¤ì›Œë“œ ê±´ë„ˆë›°ê¸° / [open] ìƒˆë¡œê³ ì¹¨")
        try:
            cmd = input("> ").strip().lower()
        except KeyboardInterrupt:
            raise
        except EOFError:
            cmd = ""

        if cmd == "skip":
            raise RuntimeError("USER_SKIPPED")

        if cmd == "open" and expected_url:
            try:
                driver.get(expected_url)
                time.sleep(1.0)
            except Exception:
                pass

        if cmd == "force":
            print("âœ… ì‚¬ìš©ì ê°•ì œ ì§„í–‰ ì„ íƒ â†’ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            return True

        if not is_login_redirect(driver) and not is_global_captcha_page(driver):
            if "search.shopping.naver.com" in (driver.current_url or ""):
                if search_dom_looks_ok(driver):
                    print("âœ… ì •ìƒ DOM í™•ì¸ â†’ ìˆ˜ì§‘ ì¬ê°œ")
                    return True
                else:
                    print("â€¦ ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ DOMì´ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    continue
            else:
                print("âœ… ë¬¸ì œ í•´ì œ â†’ ìˆ˜ì§‘ ì¬ê°œ")
                return True

        print("âš ï¸ ì—¬ì „íˆ ìº¡ì°¨/ë¡œê·¸ì¸ ìƒíƒœë¡œ ë³´ì…ë‹ˆë‹¤.")

# ============ ì•ˆì „ ì´ë™ ============
def safe_get(driver, url: str, wait_complete_seconds: int = PAGE_WAIT,
             auto_resume: bool = True, poll_interval: float = 3.0,
             poll_timeout: int = 600, interactive: bool = False):
    url = normalize_url(url) or url
    for attempt in range(1, SAFE_GET_MAX_RETRIES + 1):
        driver.get(url)
        try:
            WebDriverWait(driver, wait_complete_seconds).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
        except Exception:
            pass

        # ìƒíƒœ ì²˜ë¦¬
        if is_login_redirect(driver) or is_global_captcha_page(driver):
            reason = "ë¡œê·¸ì¸" if is_login_redirect(driver) else "ìº¡ì°¨"
            if auto_resume and not interactive:
                ok = auto_wait_issue_clear(driver, expected_url=url, reason=reason,
                                           poll_interval=poll_interval, poll_timeout=poll_timeout)
                if not ok:
                    # ìë™ ì‹¤íŒ¨ â†’ ë§ˆì§€ë§‰ì— í•œë²ˆ ë” ìˆ˜ë™ ì‹œë„ ê°€ëŠ¥
                    print("ìë™ ì¬ê°œ ì‹¤íŒ¨ â†’ ìˆ˜ë™ ëª¨ë“œ ì‹œë„")
                    ok = prompt_wait_issue_clear(driver, expected_url=url, reason=reason)
                if ok:
                    # í•´ì œëœ ìƒíƒœì—ì„œ ë£¨í”„ ë§ˆë¬´ë¦¬
                    pass
                else:
                    raise RuntimeError("AUTO_RESUME_FAILED")
            else:
                # ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ
                ok = prompt_wait_issue_clear(driver, expected_url=url, reason=reason)
                if not ok:
                    raise RuntimeError("USER_ABORT")

        time.sleep(SAFE_GET_STABILIZE_SLEEP)

        # ì•ˆì •í™” íŒì •
        if "search.shopping.naver.com" in (driver.current_url or ""):
            if search_dom_looks_ok(driver):
                return
        else:
            if not (is_login_redirect(driver) or is_global_captcha_page(driver)):
                return

        print(f"â€¦ í˜ì´ì§€ ì•ˆì •í™” ì¬ì‹œë„ {attempt}/{SAFE_GET_MAX_RETRIES}")
        time.sleep(0.7)

    # ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ë¬¸ì œë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©ì ê°œì… ìš”ì²­
    if is_login_redirect(driver) or is_global_captcha_page(driver):
        reason = "ë¡œê·¸ì¸" if is_login_redirect(driver) else "ìº¡ì°¨"
        if auto_resume and not interactive:
            ok = auto_wait_issue_clear(driver, expected_url=url, reason=reason,
                                       poll_interval=poll_interval, poll_timeout=poll_timeout)
            if ok:
                return
        # ê·¸ë˜ë„ ì•ˆë˜ë©´ ìˆ˜ë™ìœ¼ë¡œ í•œ ë²ˆ ë”
        prompt_wait_issue_clear(driver, expected_url=url, reason=reason)

# ============ ë“œë¼ì´ë²„ ============
def make_driver(headless=False):
    opt = uc.ChromeOptions()
    opt.add_argument("--disable-popup-blocking")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-blink-features=AutomationControlled")
    opt.add_argument(f"--user-data-dir={USER_DATA_DIR}")
    opt.add_argument("--profile-directory=Default")
    opt.add_argument("--disable-features=SameSiteByDefaultCookies,CookiesWithoutSameSiteMustBeSecure")
    if headless:
        opt.add_argument("--headless=new")
    d = uc.Chrome(options=opt)
    d.set_page_load_timeout(45)
    return d

# ============ ìŠ¤í¬ë¡¤ & ìˆ˜ì§‘ ============
def scroll_full_page(driver, max_steps=SCROLL_MAX_STEPS, pause=SCROLL_PAUSE):
    last_h = -1
    same_count = 0
    for _ in range(max_steps):
        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        except WebDriverException:
            pass
        time.sleep(pause)
        try:
            h = driver.execute_script("return document.body.scrollHeight;")
        except WebDriverException:
            h = last_h
        if h == last_h:
            same_count += 1
            if same_count >= 2:
                break
        else:
            same_count = 0
        last_h = h
    time.sleep(SCROLL_SETTLE_PAUSE)

def js_collect_candidates(driver) -> List[str]:
    script = r"""
    const urls = new Set();
    document.querySelectorAll('a[href]').forEach(a => {
      let u = a.getAttribute('href') || '';
      if (u) urls.add(u);
    });
    const ATTRS = ['data-href','data-url','data-link','data-mall-url','data-nv-mall-url','data-nv-url'];
    ATTRS.forEach(attr => {
      document.querySelectorAll(`[${attr}]`).forEach(el => {
        let v = el.getAttribute(attr);
        if (v) urls.add(v);
      });
    });
    document.querySelectorAll('[onclick]').forEach(el => {
      let v = el.getAttribute('onclick') || '';
      const m = v.match(/https?:\/\/[^\s"'<>]+/g);
      if (m) m.forEach(x => urls.add(x));
    });
    document.querySelectorAll('[role="link"]').forEach(el => {
      let v = el.getAttribute('href') || el.getAttribute('data-href') || el.getAttribute('data-url');
      if (v) urls.add(v);
    });
    return Array.from(urls);
    """
    try:
        return driver.execute_script(script)
    except JavascriptException:
        return []

def extract_smartstores_from_sources(sources: Iterable[str]) -> Set[str]:
    stores: Set[str] = set()
    for href in sources:
        if not href: continue
        href = html.unescape(href)
        lo = href.lower()
        if "/adcr" in lo:
            continue
        base = extract_from_outlink(href)
        if not base and "smartstore.naver.com" in lo:
            base = to_store_base(href)
        if base:
            stores.add(base)
    return stores

def collect_from_current_page(driver) -> List[str]:
    scroll_full_page(driver)
    dom_candidates = js_collect_candidates(driver)
    dom_stores = extract_smartstores_from_sources(dom_candidates)
    src = driver.page_source or ""
    src_candidates = []
    for m in OUTLINK_RE.finditer(src):
        src_candidates.append(f"https://smartstore.naver.com/inflow/outlink/url?url={m.group(1)}")
    for m in SMARTSTORE_BASE_RE.finditer(src):
        src_candidates.append(m.group(0))
    src_stores = extract_smartstores_from_sources(src_candidates)
    all_stores = sorted(dom_stores | src_stores)
    print(f"  - DOMí›„ë³´:{len(dom_candidates)} / DOMìŠ¤í† ì–´:{len(dom_stores)} / HTMLìŠ¤í† ì–´:{len(src_stores)}")
    return all_stores

# ============ ì¬í•˜ì´ë“œë ˆì´ì…˜ ============
def ensure_results_hydrated(driver, page_url: str, attempts: int = 2) -> bool:
    for attempt in range(1, attempts + 1):
        try:
            try:
                ActionChains(driver).move_by_offset(10, 10).perform()
            except Exception:
                pass
            try:
                ActionChains(driver).send_keys(Keys.PAGE_DOWN).perform()
                time.sleep(0.2)
                ActionChains(driver).send_keys(Keys.PAGE_UP).perform()
            except Exception:
                pass
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(0.2)
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                driver.execute_script("window.dispatchEvent(new Event('scroll'));")
                time.sleep(0.4)
            try:
                candidates = driver.find_elements(By.XPATH, "//a[contains(@role,'tab') or contains(@class,'_filter') or contains(@class,'tab')]")
                if candidates:
                    try:
                        driver.execute_script("arguments[0].click();", candidates[0])
                    except Exception:
                        candidates[0].click()
                    time.sleep(0.6)
            except Exception:
                pass
            time.sleep(0.8)
            if search_dom_looks_ok(driver):
                anchors = driver.find_elements(By.XPATH, "//a[contains(@href,'smartstore.naver.com')]")
                if anchors:
                    return True
            if attempt == attempts:
                driver.refresh()
                time.sleep(1.0)
                if search_dom_looks_ok(driver):
                    anchors = driver.find_elements(By.XPATH, "//a[contains(@href,'smartstore.naver.com')]")
                    if anchors:
                        return True
        except Exception:
            time.sleep(0.5)
    return False

def collect_with_rehydrate(driver, page_url: str) -> List[str]:
    stores = collect_from_current_page(driver)
    if len(stores) > 0:
        return stores
    has_many_candidates = False
    try:
        cands = driver.find_elements(By.CSS_SELECTOR, 'a[href], [data-href], [data-url]')
        has_many_candidates = len(cands) >= 80
    except Exception:
        pass
    hydrated = ensure_results_hydrated(driver, page_url, attempts=2)
    if hydrated or has_many_candidates:
        stores_retry = collect_from_current_page(driver)
        if len(stores_retry) > 0:
            print("  â†» ì¬í•˜ì´ë“œë ˆì´ì…˜ í›„ ì¬ìˆ˜ì§‘ ì„±ê³µ")
            return stores_retry
    return stores

# ============ ë¡œê·¸ì¸ & ì´ë™ ============
def go_login_then_return(driver, target_url: str, auto_resume: bool, poll_interval: float,
                         poll_timeout: int, interactive: bool):
    login_url = LOGIN_URL_TMPL.format(return_url=quote(normalize_url(target_url) or target_url, safe=""))
    print("ğŸ” ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™:", login_url)
    safe_get(driver, login_url, auto_resume=auto_resume, poll_interval=poll_interval,
             poll_timeout=poll_timeout, interactive=interactive)
    print("â†©ï¸ ë¡œê·¸ì¸ í›„ íƒ€ê²Ÿ URL ì•ˆì •í™” í™•ì¸:", target_url)
    safe_get(driver, target_url, auto_resume=auto_resume, poll_interval=poll_interval,
             poll_timeout=poll_timeout, interactive=interactive)

# ============ í‚¤ì›Œë“œ(ê²€ìƒ‰ URL) ë‹¨ìœ„ ìˆ˜ì§‘ ============
def make_driver(headless=False):
    opt = uc.ChromeOptions()
    opt.add_argument("--disable-popup-blocking")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-blink-features=AutomationControlled")
    opt.add_argument(f"--user-data-dir={USER_DATA_DIR}")
    opt.add_argument("--profile-directory=Default")
    opt.add_argument("--disable-features=SameSiteByDefaultCookies,CookiesWithoutSameSiteMustBeSecure")
    if headless:
        opt.add_argument("--headless=new")
    d = uc.Chrome(options=opt)
    d.set_page_load_timeout(45)
    return d

def collect_one_search(idx: int, start_url: str, pages: int, headless: bool,
                       no_login_first: bool, stores_txt_path: Path,
                       auto_resume: bool, poll_interval: float, poll_timeout: int,
                       interactive: bool) -> Set[str]:
    stores: Set[str] = set()
    url = normalize_url(start_url) or start_url
    driver = make_driver(headless=headless)
    try:
        if not no_login_first:
            go_login_then_return(driver, url, auto_resume=auto_resume,
                                 poll_interval=poll_interval, poll_timeout=poll_timeout,
                                 interactive=interactive)
        for p in range(1, pages + 1):
            page_url = with_paging_index(url, p)
            print(f"ğŸ“„ [{idx+1}] í˜ì´ì§€ {p}/{pages}: {page_url}")
            safe_get(driver, page_url, auto_resume=auto_resume,
                     poll_interval=poll_interval, poll_timeout=poll_timeout,
                     interactive=interactive)
            page_stores = collect_with_rehydrate(driver, page_url)
            print(f"  â†’ ìˆ˜ì§‘: {len(page_stores)}ê°œ")
            stores.update(page_stores)
            if stores_txt_path:
                try:
                    with stores_txt_path.open("a", encoding="utf-8") as f:
                        for s in page_stores:
                            f.write(s + "\n")
                except Exception as e:
                    print(f"ë¶€ë¶„ ì €ì¥ ì‹¤íŒ¨: {e}")
    except KeyboardInterrupt:
        print("â¸ï¸ í˜ì´ì§€ ë£¨í”„ ì¤‘ë‹¨ë¨(KeyboardInterrupt). í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ë¶„ ë°˜í™˜.")
        return stores
    finally:
        try: driver.quit()
        except Exception: pass
    return stores

# ============ ë©”ì¸ ============
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--url-file", required=True)
    ap.add_argument("--pages", type=int, default=5)
    ap.add_argument("--out", default="stores_unique.txt")
    ap.add_argument("--headless", action="store_true")
    ap.add_argument("--no-login-first", action="store_true",
                    help="í‚¤ì›Œë“œ ì‹œì‘ ì‹œ ë¡œê·¸ì¸ ì„ í–‰ ë¹„í™œì„±í™”")
    # ìë™ ì¬ê°œ ê´€ë ¨
    ap.add_argument("--interactive", action="store_true",
                    help="ìº¡ì°¨/ë¡œê·¸ì¸ ì‹œ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ ì‚¬ìš©")
    ap.add_argument("--poll-interval", type=float, default=3.0,
                    help="ìë™ ì¬ê°œ í´ë§ ê°„ê²©(ì´ˆ)")
    ap.add_argument("--poll-timeout", type=int, default=600,
                    help="ìë™ ì¬ê°œ íƒ€ì„ì•„ì›ƒ(ì´ˆ)")
    args = ap.parse_args()

    url_lines = [line.strip() for line in open(args.url_file, encoding="utf-8") if line.strip()]
    if not url_lines:
        print("URL ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."); return
    print(f"ì´ í‚¤ì›Œë“œ(ê²€ìƒ‰ URL): {len(url_lines)}")

    all_unique: Set[str] = set()

    for idx, start_url in enumerate(url_lines):
        done_path, stores_txt_path, base = paths_for_url(idx, start_url)
        if done_path.exists():
            print(f"\n=== [{idx+1}/{len(url_lines)}] ì´ë¯¸ ì™„ë£Œ(.done) â†’ ê±´ë„ˆëœ€: {start_url}")
            if stores_txt_path.exists():
                try:
                    for line in stores_txt_path.read_text(encoding="utf-8").splitlines():
                        if line.strip(): all_unique.add(line.strip())
                except Exception as e:
                    print(f"ë¶€ë¶„ ê²°ê³¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            continue

        print(f"\n=== [{idx+1}/{len(url_lines)}] ìˆ˜ì§‘ ì‹œì‘ ===")
        print(f"í‚¤ì›Œë“œ URL: {start_url}")
        if stores_txt_path.exists():
            try: stores_txt_path.unlink()
            except Exception: pass

        try:
            stores = collect_one_search(
                idx, start_url, pages=args.pages, headless=args.headless,
                no_login_first=args.no_login_first, stores_txt_path=stores_txt_path,
                auto_resume=(not args.interactive),
                poll_interval=args.poll_interval, poll_timeout=args.poll_timeout,
                interactive=args.interactive
            )
        except RuntimeError as e:
            if str(e) == "USER_SKIPPED":
                print("â†· ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì´ í‚¤ì›Œë“œ ê±´ë„ˆëœ€.")
                stores = set()
            else:
                print(f"âš ï¸ ëŸ°íƒ€ì„ ì˜ˆì™¸: {e}")
                break
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€ â†’ í˜„ì¬ URLê¹Œì§€ ë¶€ë¶„ ê²°ê³¼ ë°˜ì˜ í›„ ì¢…ë£Œ")
            break
        except Exception as e:
            print(f"âš ï¸ ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            break

        all_unique.update(stores)
        try:
            done_path.write_text("done", encoding="utf-8")
        except Exception as e:
            print(f".done íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")

        print(f"â†’ í‚¤ì›Œë“œ ì™„ë£Œ. ëˆ„ì  ê³ ìœ  ìŠ¤í† ì–´ ìˆ˜: {len(all_unique)}")
        try:
            Path(args.out).write_text("\n".join(sorted(all_unique)) + "\n", encoding="utf-8")
            print(f"  (ì¤‘ê°„ ì €ì¥) {args.out}")
        except Exception as e:
            print(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ì¢…ë£Œ ì‹œ ë¶€ë¶„ê²°ê³¼ ë³‘í•©
    try:
        for p in PROGRESS_DIR.glob("*.stores.txt"):
            try:
                for line in p.read_text(encoding="utf-8").splitlines():
                    if line.strip(): all_unique.add(line.strip())
            except Exception:
                pass
    except Exception:
        pass

    final_sorted = sorted(all_unique)
    try:
        Path(args.out).write_text("\n".join(final_sorted) + "\n", encoding="utf-8")
        print(f"\nâœ… ìµœì¢… ì €ì¥: {args.out} (ê³ ìœ  ìŠ¤í† ì–´ {len(final_sorted)}ê°œ)")
    except Exception as e:
        print(f"\nâš ï¸ ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
    print("ì‘ì—… ì¢…ë£Œ")

if __name__ == "__main__":
    main()
