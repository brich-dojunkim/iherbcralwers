"""
ì¿ íŒ¡ í¬ë¡¤ë§ ê´€ë¦¬ì
"""

import sys
from datetime import datetime
from settings import COUPANG_PATH, UPDATER_CONFIG

# ì¿ íŒ¡ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(COUPANG_PATH))

try:
    from crawler import CoupangCrawlerMacOS
    COUPANG_AVAILABLE = True
    print("âœ… ì¿ íŒ¡ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ì¿ íŒ¡ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    COUPANG_AVAILABLE = False


class CoupangManager:
    """ì¿ íŒ¡ í¬ë¡¤ë§ ì „ë‹´ ê´€ë¦¬ì"""
    
    def __init__(self, headless=False):
        if not COUPANG_AVAILABLE:
            raise ImportError("ì¿ íŒ¡ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        self.headless = headless
        self.crawler = None
        self.brand_urls = UPDATER_CONFIG['BRAND_SEARCH_URLS']
        self.delay_range = UPDATER_CONFIG['DELAY_RANGE']
    
    def crawl_brand_products(self, brand_name):
        """ë¸Œëœë“œë³„ ìƒí’ˆ í¬ë¡¤ë§"""
        if brand_name not in self.brand_urls:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¸Œëœë“œ: {brand_name}")
        
        search_url = self.brand_urls[brand_name]
        print(f"ğŸ¤– ì¿ íŒ¡ í¬ë¡¤ë§ ì‹œì‘: {brand_name}")
        
        products = []
        try:
            self.crawler = CoupangCrawlerMacOS(
                headless=self.headless,
                delay_range=self.delay_range,
                download_images=True
            )
            
            if not self.crawler.start_driver():
                raise Exception("ì¿ íŒ¡ í¬ë¡¤ëŸ¬ ì‹œì‘ ì‹¤íŒ¨")
            
            products = self.crawler.crawl_all_pages(search_url)
            print(f"ğŸ“¡ í¬ë¡¤ë§ ì™„ë£Œ: {len(products)}ê°œ")
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        finally:
            self.close()
        
        return products
    
    def update_existing_products(self, existing_df, new_products):
        """ê¸°ì¡´ ìƒí’ˆ ê°€ê²© ì—…ë°ì´íŠ¸"""
        existing_ids = set(str(pid) for pid in existing_df['coupang_product_id'].dropna())
        crawled_dict = {str(p['product_id']): p for p in new_products if p.get('product_id')}
        
        updated_count = 0
        date_suffix = datetime.now().strftime("_%Y%m%d")
        
        for idx, row in existing_df.iterrows():
            product_id = str(row.get('coupang_product_id', ''))
            
            if product_id in crawled_dict:
                new_product = crawled_dict[product_id]
                existing_df.at[idx, f'ì¿ íŒ¡í˜„ì¬ê°€ê²©{date_suffix}'] = new_product.get('current_price', '')
                existing_df.at[idx, f'ì¿ íŒ¡ì •ê°€{date_suffix}'] = new_product.get('original_price', '')
                existing_df.at[idx, f'ì¿ íŒ¡í• ì¸ìœ¨{date_suffix}'] = new_product.get('discount_rate', '')
                existing_df.at[idx, f'ì¿ íŒ¡ë¦¬ë·°ìˆ˜{date_suffix}'] = new_product.get('review_count', '')
                existing_df.at[idx, f'ì¿ íŒ¡í‰ì {date_suffix}'] = new_product.get('rating', '')
                existing_df.at[idx, f'í¬ë¡¤ë§ì¼ì‹œ{date_suffix}'] = datetime.now().isoformat()
                existing_df.at[idx, 'update_status'] = 'UPDATED'
                updated_count += 1
            else:
                existing_df.at[idx, 'update_status'] = 'NOT_FOUND'
        
        return existing_df, updated_count
    
    def find_new_products(self, existing_df, crawled_products):
        """ì‹ ê·œ ìƒí’ˆ ë°œê²¬"""
        existing_ids = set(str(pid) for pid in existing_df['coupang_product_id'].dropna())
        crawled_dict = {str(p['product_id']): p for p in crawled_products if p.get('product_id')}
        
        new_products = []
        for product_id, product in crawled_dict.items():
            if product_id not in existing_ids:
                new_products.append(product)
        
        return new_products
    
    def close(self):
        """í¬ë¡¤ëŸ¬ ì •ë¦¬"""
        if self.crawler:
            self.crawler.close()
            self.crawler = None