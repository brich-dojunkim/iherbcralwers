"""
ì¿ íŒ¡ í¬ë¡¤ëŸ¬ - DB ë²„ì „
CSV ì €ì¥ ëŒ€ì‹  SQLite DBì— ì§ì ‘ ì €ì¥
"""

import sys
import os

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from coupang_manager import BrowserManager
from scraper import ProductScraper
from image_downloader import ImageDownloader
from page_navigator import PageNavigator
from datetime import datetime
from db import Database
from config import PathConfig


class CoupangCrawlerDB:
    """DB ì—°ë™ ì¿ íŒ¡ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, db: Database, brand_name: str, 
                 headless: bool = False, 
                 delay_range: tuple = (2, 5),
                 download_images: bool = True):
        """
        Args:
            db: Database ì¸ìŠ¤í„´ìŠ¤
            brand_name: ë¸Œëœë“œëª…
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
            delay_range: ë”œë ˆì´ ë²”ìœ„
            download_images: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
        """
        self.db = db
        self.brand_name = brand_name
        self.headless = headless
        self.delay_range = delay_range
        self.download_images = download_images
        
        # ê¸°ì¡´ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.browser = BrowserManager(headless)
        self.scraper = ProductScraper()
        self.image_downloader = ImageDownloader() if download_images else None
        self.navigator = PageNavigator(self.browser, self.scraper, self.image_downloader)
        
        self.crawled_count = 0
        self.new_products = 0
        self.updated_products = 0
    
    def start_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì‹œì‘"""
        return self.browser.start_driver()
    
    def crawl_and_save(self, search_url: str, max_pages: int = None) -> dict:
        """
        í¬ë¡¤ë§ ë° DB ì €ì¥
        
        Args:
            search_url: ì¿ íŒ¡ ê²€ìƒ‰ URL
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*80}")
        print(f"ğŸ¯ DB ì—°ë™ ì¿ íŒ¡ í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*80}")
        print(f"ë¸Œëœë“œ: {self.brand_name}")
        print(f"ì´ë¯¸ì§€: {'í™œì„±í™”' if self.download_images else 'ë¹„í™œì„±í™”'}")
        print(f"{'='*80}\n")
        
        # ë“œë¼ì´ë²„ ì‹œì‘
        if not self.start_driver():
            print("âŒ ë“œë¼ì´ë²„ ì‹œì‘ ì‹¤íŒ¨")
            return self._get_stats()
        
        try:
            # ê¸°ì¡´ ìƒí’ˆ ìˆ˜ í™•ì¸
            initial_stats = self.db.get_brand_stats(self.brand_name)
            initial_count = initial_stats.get('total_products', 0)
            
            print(f"ğŸ“Š ê¸°ì¡´ ìƒí’ˆ: {initial_count}ê°œ\n")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            products = self.navigator.crawl_all_pages(
                search_url, 
                max_pages, 
                self.delay_range
            )
            
            if not products:
                print("âš ï¸ í¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤")
                return self._get_stats()
            
            print(f"\n{'='*80}")
            print(f"ğŸ’¾ DB ì €ì¥ ì‹œì‘: {len(products)}ê°œ")
            print(f"{'='*80}")
            
            # DB ì €ì¥
            for idx, product_data in enumerate(products, 1):
                try:
                    self._save_product(product_data, idx, len(products))
                except Exception as e:
                    print(f"  âŒ ìƒí’ˆ {idx} ì €ì¥ ì‹¤íŒ¨: {e}")
                    continue
            
            # ë¸Œëœë“œ í¬ë¡¤ë§ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.db.update_brand_crawled(self.brand_name)
            
            # ìµœì¢… í†µê³„
            self._print_summary(initial_count)
            
            # ì‚¬ë¼ì§„ ìƒí’ˆ ê°ì§€
            self._detect_missing_products()
            
            return self._get_stats()
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
            self.db.update_brand_crawled(self.brand_name)
            return self._get_stats()
            
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return self._get_stats()
            
        finally:
            self.close()
    
    def _save_product(self, product_data: dict, idx: int, total: int):
        """ë‹¨ì¼ ìƒí’ˆ DB ì €ì¥"""
        # product_id ì¶”ì¶œ
        product_id = self.db.insert_crawled_product(
            self.brand_name, 
            product_data
        )
        
        self.crawled_count += 1
        
        # ì‹ ê·œ vs ì—…ë°ì´íŠ¸ íŒë‹¨ (ê°„ë‹¨í•œ ë°©ë²•)
        # ì‹¤ì œë¡œëŠ” product_idê°€ ìƒˆë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•˜ì§€ë§Œ,
        # insert_crawled_product()ê°€ í•­ìƒ idë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ
        # ì—¬ê¸°ì„œëŠ” ì¹´ìš´íŠ¸ë§Œ ì¦ê°€
        
        # ì§„í–‰ë¥  í‘œì‹œ (10ê°œë§ˆë‹¤)
        if idx % 10 == 0 or idx == total:
            print(f"  ğŸ’¾ ì €ì¥ ì¤‘: {idx}/{total} ({idx/total*100:.1f}%)")
    
    def _print_summary(self, initial_count: int):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½")
        print(f"{'='*80}")
        
        final_stats = self.db.get_brand_stats(self.brand_name)
        final_count = final_stats.get('total_products', 0)
        
        print(f"ì´ í¬ë¡¤ë§: {self.crawled_count}ê°œ")
        print(f"DB ìƒí’ˆ ìˆ˜: {initial_count}ê°œ â†’ {final_count}ê°œ")
        
        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„
        by_stage = final_stats.get('by_stage', {})
        print(f"\níŒŒì´í”„ë¼ì¸ ë‹¨ê³„:")
        for stage, count in by_stage.items():
            emoji = {
                'crawled': 'ğŸ†•',
                'translated': 'ğŸ“',
                'matched': 'âœ…',
                'failed': 'âŒ'
            }.get(stage, 'â“')
            print(f"  {emoji} {stage}: {count}ê°œ")
        
        # ì´ë¯¸ì§€ í†µê³„
        if self.image_downloader:
            stats = self.image_downloader.image_download_stats
            print(f"\nì´ë¯¸ì§€ ìˆ˜ì§‘:")
            print(f"  ğŸ“¸ ì„±ê³µ: {stats['successful_downloads']}ê°œ")
            print(f"  â­ï¸ ê¸°ì¡´: {stats['skipped_existing']}ê°œ")
            print(f"  âŒ ì‹¤íŒ¨: {stats['failed_downloads']}ê°œ")
    
    def _detect_missing_products(self):
        """ì‚¬ë¼ì§„ ìƒí’ˆ ê°ì§€"""
        print(f"\n{'='*80}")
        print(f"ğŸ” ì‚¬ë¼ì§„ ìƒí’ˆ ê°ì§€")
        print(f"{'='*80}")
        
        missing = self.db.get_missing_products(self.brand_name)
        
        if missing:
            print(f"âš ï¸ ë°œê²¬: {len(missing)}ê°œ ìƒí’ˆì´ ìµœì‹  í¬ë¡¤ë§ì—ì„œ ì œì™¸ë¨")
            print(f"\nìƒí’ˆ ì˜ˆì‹œ:")
            for product in missing[:5]:
                print(f"  - {product['coupang_product_name'][:50]}...")
            
            if len(missing) > 5:
                print(f"  ... ì™¸ {len(missing) - 5}ê°œ")
        else:
            print(f"âœ“ ëª¨ë“  ìƒí’ˆì´ í¬ë¡¤ë§ì— í¬í•¨ë¨")
    
    def _get_stats(self) -> dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            'crawled_count': self.crawled_count,
            'new_products': self.new_products,
            'updated_products': self.updated_products
        }
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        self.browser.close()


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª DB ì—°ë™ ì¿ íŒ¡ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸\n")
    
    # DB ì´ˆê¸°í™”
    db_path = os.path.join(PathConfig.DATA_ROOT, "products.db")
    db = Database(db_path)
    
    # ë¸Œëœë“œ ë“±ë¡
    brand_name = "thorne"
    search_url = "https://www.coupang.com/np/search?listSize=36&filterType=coupang_global&rating=0&isPriceRange=false&minPrice=&maxPrice=&component=&sorter=scoreDesc&brand=14420&offerCondition=&filter=194176%23attr_7652%2431823%40DEFAULT&fromComponent=N&channel=user&selectedPlpKeepFilter=&q=thorne"
    
    db.upsert_brand(brand_name, search_url)
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = CoupangCrawlerDB(
        db=db,
        brand_name=brand_name,
        headless=False,
        delay_range=(3, 6),
        download_images=True
    )
    
    try:
        stats = crawler.crawl_and_save(
            search_url=search_url,
            max_pages=2  # í…ŒìŠ¤íŠ¸ìš© 2í˜ì´ì§€ë§Œ
        )
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"í¬ë¡¤ë§: {stats['crawled_count']}ê°œ")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
    finally:
        crawler.close()


if __name__ == "__main__":
    main()