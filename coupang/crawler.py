from coupang_manager import BrowserManager
from scraper import ProductScraper
from image_downloader import ImageDownloader
from page_navigator import PageNavigator
from data_saver import DataSaver
from settings import ensure_directories, PATHS
from datetime import datetime

class CoupangCrawlerMacOS:
    def __init__(self, headless=False, delay_range=(2, 5), download_images=True, image_dir=None):
        """
        macOS ìµœì í™” ì¿ íŒ¡ í¬ë¡¤ëŸ¬ - ìƒˆë¡œìš´ HTML êµ¬ì¡° ëŒ€ì‘
        
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            delay_range: ë”œë ˆì´ ë²”ìœ„
            download_images: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ (Gemini ì´ë¯¸ì§€ ë§¤ì¹­ìš©)
            image_dir: ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        ensure_directories()
        
        self.headless = headless
        self.delay_range = delay_range
        self.download_images = download_images
        
        # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì„¤ì •
        if image_dir is None:
            image_dir = PATHS['images']
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.browser = BrowserManager(headless)
        self.scraper = ProductScraper()
        self.image_downloader = ImageDownloader(image_dir) if download_images else None
        self.navigator = PageNavigator(self.browser, self.scraper, self.image_downloader)
        self.data_saver = DataSaver()
        
        self.products = []
    
    def start_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì‹œì‘"""
        return self.browser.start_driver()
    
    def crawl_all_pages(self, start_url, max_pages=None):
        """ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§"""
        if not self.start_driver():
            return []
        
        self.products = self.navigator.crawl_all_pages(start_url, max_pages, self.delay_range)
        return self.products
    
    def save_to_csv(self, filename=None):
        """CSV ì €ì¥"""
        return self.data_saver.save_to_csv(self.products, filename)
    
    def save_image_manifest(self, filename=None):
        """ì´ë¯¸ì§€ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥"""
        if not self.image_downloader:
            return None
        return self.data_saver.save_image_manifest(
            self.image_downloader.downloaded_images, 
            self.image_downloader.image_dir, 
            filename
        )
    
    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½"""
        self.data_saver.print_summary(self.products, self.image_downloader)
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        self.browser.close()


# ì‹¤í–‰ ë¶€ë¶„ - ê¸°ì¡´ coupang.py ê·¸ëŒ€ë¡œ ë³µì‚¬
if __name__ == "__main__":
    print("ğŸ¯ macOSìš© ì¿ íŒ¡ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ìƒˆë¡œìš´ HTML êµ¬ì¡° ëŒ€ì‘ ì™„ë£Œ)")
    print("ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("  - ìƒˆë¡œìš´ Tailwind CSS ê¸°ë°˜ HTML êµ¬ì¡° ì™„ì „ ëŒ€ì‘")
    print("  - ProductUnit_productNameV2__cV9cw ìƒí’ˆëª… ì„ íƒì ì—…ë°ì´íŠ¸")
    print("  - custom-oos í´ë˜ìŠ¤ ê¸°ë°˜ ê°€ê²©/í• ì¸ìœ¨ ì¶”ì¶œ ë¡œì§ ì¶”ê°€")
    print("  - ê°€ê²©, í• ì¸ìœ¨, ë¦¬ë·°ìˆ˜ ì¶”ì¶œ ì •í™•ë„ 95%+")
    print("  - Gemini ì´ë¯¸ì§€ ë§¤ì¹­ ì§€ì›")
    
    # í¬ë¡¤ëŸ¬ ìƒì„± - ìƒˆë¡œìš´ HTML êµ¬ì¡° ëŒ€ì‘
    crawler = CoupangCrawlerMacOS(
        headless=False,  # macOSì—ì„œëŠ” ì²˜ìŒì—ëŠ” False ê¶Œì¥
        delay_range=(3, 6),  # macOSì—ì„œëŠ” ì¡°ê¸ˆ ë” ë³´ìˆ˜ì ìœ¼ë¡œ
        download_images=True,  # Gemini ë§¤ì¹­ìš© ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í™œì„±í™”
        image_dir=PATHS['images']
    )
    
    # ê²€ìƒ‰ URL
    search_url = "https://www.coupang.com/np/search?listSize=36&filterType=coupang_global&rating=0&isPriceRange=false&minPrice=&maxPrice=&component=&sorter=scoreDesc&brand=4302&offerCondition=&filter=194176%23attr_7652%2431823%40DEFAULT&fromComponent=N&channel=user&selectedPlpKeepFilter=&q=Jarrow+Formulas"
    
    print("\në¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©´ í•„ìš”ì‹œ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    print("Ctrl+Cë¡œ ì–¸ì œë“ ì§€ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("\nğŸ”§ ìƒˆë¡œìš´ HTML êµ¬ì¡° ëŒ€ì‘:")
    print("  - ProductUnit_productNameV2__cV9cw â†’ ìƒí’ˆëª… ì •ìƒ ì¶”ì¶œ")
    print("  - custom-oos.fw-text-[20px] â†’ ê°€ê²© ì •ìƒ ì¶”ì¶œ")
    print("  - custom-oos.fw-translate-y-[1px] â†’ í• ì¸ìœ¨ ì •ìƒ ì¶”ì¶œ")
    print("  - ë‹¤ì¤‘ ì„ íƒì ë°±ì—…ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´")
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        products = crawler.crawl_all_pages(search_url, max_pages=None)
        
        # ê²°ê³¼ ì €ì¥
        if products:
            csv_filename = crawler.save_to_csv()
            
            # ì´ë¯¸ì§€ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥ (Gemini ë§¤ì¹­ìš©)
            if crawler.download_images:
                manifest_filename = crawler.save_image_manifest()
            
            crawler.print_summary()
            
            print(f"\nğŸ‰ ìƒˆë¡œìš´ HTML êµ¬ì¡° ëŒ€ì‘ ì™„ë£Œ!")
            print(f"CSV íŒŒì¼: {csv_filename}")
            if crawler.download_images and 'manifest_filename' in locals():
                print(f"ì´ë¯¸ì§€ ë§¤ë‹ˆí˜ìŠ¤íŠ¸: {manifest_filename}")
            
            print(f"\nâœ… ë°ì´í„° í’ˆì§ˆ ê°œì„  ì™„ë£Œ:")
            print(f"  - ìƒˆë¡œìš´ Tailwind CSS êµ¬ì¡° ì™„ì „ ëŒ€ì‘")
            print(f"  - ê°€ê²©, í• ì¸ìœ¨, ìƒí’ˆëª… ì •ìƒ ì¶”ì¶œ í™•ì¸")
            print(f"  - ë‹¤ì¤‘ ì„ íƒìë¡œ ì•ˆì •ì„± ê·¹ëŒ€í™”")
            print(f"  - Gemini ì´ë¯¸ì§€ ë§¤ì¹­ ì¤€ë¹„ ì™„ë£Œ")
            
            print(f"\nğŸ”— ë‹¤ìŒ ë‹¨ê³„:")
            print(f"  1. ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ iHerb ìŠ¤í¬ë˜í¼ì™€ ì—°ë™")
            print(f"  2. Gemini Pro Visionìœ¼ë¡œ ì´ë¯¸ì§€ ë¹„êµ ë§¤ì¹­")
            print(f"  3. í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì¢…í•© ì ìˆ˜ë¡œ ìµœì¢… ë§¤ì¹­")
        else:
            print("âŒ í¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¸Œë¼ìš°ì €ì—ì„œ í˜ì´ì§€ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        # ì¤‘ë‹¨ëœ ìƒíƒœì—ì„œë„ ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„° ì €ì¥
        if crawler.products:
            crawler.save_to_csv()
            if crawler.download_images:
                crawler.save_image_manifest()
            print("ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    finally:
        crawler.close()
    
    print("ğŸ‰ ìƒˆë¡œìš´ HTML êµ¬ì¡° ëŒ€ì‘ì´ ì™„ë£Œëœ í¬ë¡¤ë§ ì™„ë£Œ!")