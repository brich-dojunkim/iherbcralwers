"""
íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰ê¸°
ì¿ íŒ¡ í¬ë¡¤ë§ â†’ ë²ˆì—­ â†’ ì•„ì´í—ˆë¸Œ ë§¤ì¹­ì„ í•œ ë²ˆì— ì‹¤í–‰
"""

import sys
import os
from datetime import datetime

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'coupang'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'iherbscraper'))

from db import Database
from config import PathConfig
from coupang.crawler import CoupangCrawlerDB
from coupang.translator import TranslatorDB
from iherbscraper.main import IHerbScraperDB


class Pipeline:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰"""
    
    def __init__(self, db_path=None):
        """
        Args:
            db_path: DB ê²½ë¡œ (ê¸°ë³¸ê°’: products.db)
        """
        if db_path is None:
            db_path = os.path.join(PathConfig.DATA_ROOT, "products.db")
        
        self.db = Database(db_path)
        self.start_time = None
    
    def run_brand(self, brand_name: str, search_url: str = None, 
                max_pages: int = None, headless: bool = False) -> dict:
        """
        ë¸Œëœë“œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            brand_name: ë¸Œëœë“œëª…
            search_url: ì¿ íŒ¡ ê²€ìƒ‰ URL (ì‹ ê·œ ë¸Œëœë“œë§Œ í•„ìˆ˜)
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
        """
        self.start_time = datetime.now()
        
        # ë¸Œëœë“œ í™•ì¸ ë° URL ê²°ì •
        brand = self.db.get_brand(brand_name)
        
        if brand:
            # ê¸°ì¡´ ë¸Œëœë“œ â†’ DBì˜ URL ì‚¬ìš©
            actual_url = brand['coupang_search_url']
            print(f"\nğŸ“‹ ê¸°ì¡´ ë¸Œëœë“œ: {brand_name}")
            print(f"ë§ˆì§€ë§‰ í¬ë¡¤ë§: {brand['last_crawled_at'] or 'ì—†ìŒ'}")
        elif search_url:
            # ì‹ ê·œ ë¸Œëœë“œ â†’ URL ë“±ë¡
            self.db.upsert_brand(brand_name, search_url)
            actual_url = search_url
            print(f"\nğŸ†• ì‹ ê·œ ë¸Œëœë“œ ë“±ë¡: {brand_name}")
        else:
            raise ValueError(
                f"ë¸Œëœë“œ '{brand_name}'ê°€ DBì— ì—†ìŠµë‹ˆë‹¤. "
                f"ì‹ ê·œ ë¸Œëœë“œëŠ” --url ì˜µì…˜ì´ í•„ìˆ˜ì…ë‹ˆë‹¤."
            )
        
        self.start_time = datetime.now()
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {brand_name}")
        print(f"{'='*80}")
        print(f"ì‹œì‘ ì‹œê°„: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}\n")
        
        stats = {}
        
        try:
            # ë¸Œëœë“œ ë“±ë¡
            self.db.upsert_brand(brand_name, search_url)
            
            # 1ë‹¨ê³„: ì¿ íŒ¡ í¬ë¡¤ë§
            print(f"\n{'='*80}")
            print(f"1ï¸âƒ£ ì¿ íŒ¡ í¬ë¡¤ë§")
            print(f"{'='*80}")
            
            crawler = CoupangCrawlerDB(
                db=self.db,
                brand_name=brand_name,
                headless=headless,
                download_images=True
            )
            
            crawl_stats = crawler.crawl_and_save(search_url, max_pages)
            stats['crawl'] = crawl_stats
            
            if crawl_stats['crawled_count'] == 0:
                print("\nâš ï¸ í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ - íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
                return stats
            
            # 2ë‹¨ê³„: ë²ˆì—­
            print(f"\n{'='*80}")
            print(f"2ï¸âƒ£ ë²ˆì—­")
            print(f"{'='*80}")
            
            translator = TranslatorDB(self.db)
            translate_stats = translator.translate_brand(brand_name, batch_size=10)
            stats['translate'] = translate_stats
            
            if translate_stats['translated'] == 0:
                print("\nâš ï¸ ë²ˆì—­ ê²°ê³¼ ì—†ìŒ - íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
                return stats
            
            # 3ë‹¨ê³„: ì•„ì´í—ˆë¸Œ ë§¤ì¹­
            print(f"\n{'='*80}")
            print(f"3ï¸âƒ£ ì•„ì´í—ˆë¸Œ ë§¤ì¹­")
            print(f"{'='*80}")
            
            scraper = IHerbScraperDB(
                db=self.db,
                brand_name=brand_name,
                headless=headless,
                max_products=4
            )
            
            match_stats = scraper.match_all_products(resume=True)
            stats['match'] = match_stats
            
            # ìµœì¢… ìš”ì•½
            self._print_summary(brand_name, stats)
            
            return stats
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
            self._print_summary(brand_name, stats)
            return stats
            
        except Exception as e:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return stats
            
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if 'scraper' in locals():
                scraper.close()
    
    def _print_summary(self, brand_name: str, stats: dict):
        """ìµœì¢… í†µê³„ ìš”ì•½"""
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ìš”ì•½")
        print(f"{'='*80}")
        print(f"ë¸Œëœë“œ: {brand_name}")
        print(f"ì‹¤í–‰ ì‹œê°„: {duration.total_seconds()/60:.1f}ë¶„")
        
        if 'crawl' in stats:
            print(f"\n1ï¸âƒ£ í¬ë¡¤ë§: {stats['crawl']['crawled_count']}ê°œ")
        
        if 'translate' in stats:
            print(f"2ï¸âƒ£ ë²ˆì—­: {stats['translate']['translated']}ê°œ")
        
        if 'match' in stats:
            match = stats['match']
            total = match['matched'] + match['not_found'] + match['error']
            success_rate = (match['matched'] / total * 100) if total > 0 else 0
            print(f"3ï¸âƒ£ ë§¤ì¹­: {match['matched']}/{total}ê°œ ({success_rate:.1f}%)")
        
        # DB ìµœì¢… ìƒíƒœ
        brand_stats = self.db.get_brand_stats(brand_name)
        print(f"\nğŸ“ˆ ìµœì¢… ìƒíƒœ:")
        for stage, count in brand_stats['by_stage'].items():
            emoji = {
                'crawled': 'ğŸ†•',
                'translated': 'ğŸ“',
                'matched': 'âœ…',
                'failed': 'âŒ'
            }.get(stage, 'â“')
            print(f"  {emoji} {stage}: {count}ê°œ")
        
        print(f"{'='*80}\n")


def main():
    """CLI ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description='íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰')
    parser.add_argument('--brand', required=True, help='ë¸Œëœë“œëª…')
    parser.add_argument('--url', help='ì¿ íŒ¡ ê²€ìƒ‰ URL (ì‹ ê·œ ë¸Œëœë“œë§Œ í•„ìˆ˜)')
    parser.add_argument('--max-pages', type=int, help='ìµœëŒ€ í˜ì´ì§€ ìˆ˜')
    parser.add_argument('--headless', action='store_true', help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ')
    parser.add_argument('--list', action='store_true', help='ë¸Œëœë“œ ëª©ë¡ ë³´ê¸°')
    
    args = parser.parse_args()
    
    pipeline = Pipeline()
    
    # ë¸Œëœë“œ ëª©ë¡
    if args.list:
        pipeline.list_brands()
        return 0
    
    # ì‹¤í–‰
    try:
        stats = pipeline.run_brand(
            brand_name=args.brand,
            search_url=args.url,  # None ê°€ëŠ¥
            max_pages=args.max_pages,
            headless=args.headless
        )
        return 0 if stats else 1
    except ValueError as e:
        print(f"âŒ {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())